{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paper : https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://nlp.seas.harvard.edu/2018/04/03/attention.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maps input sequences to a sequence of continuous representations $x=(x_1 \\cdots, x_n) \\rightarrow z=(z_1, \\cdots, y_n)$, given $z$ generates an output sequence $y=(y_1, \\cdots, y_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled Dot-Product Attention\"\"\"\n",
    "    def __init__(self, d_k):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = torch.tensor(d_k).float()\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        * q: (B, T_q, d_q), d_q = d_k\n",
    "        * k: (B, T_k, d_k)\n",
    "        * v: (B, T_v, d_v), T_k = T_v\n",
    "        -------------------------------\n",
    "        Outputs:\n",
    "        * output: (B, T_q, d_v)\n",
    "        * probs: (B, T_q, T_k)\n",
    "        \"\"\"\n",
    "        assert q.size(2) == k.size(2), \"d_q = d_k\"\n",
    "        assert k.size(1) == v.size(1), \"T_k = T_v\"\n",
    "        attn = torch.bmm(q, k.transpose(1, 2))  # (B, T_q, d_k) * (B, T_k, d_k) -> (B, T_q, T_k)\n",
    "        attn = attn / torch.sqrt(self.d_k)\n",
    "        # why doing this? \n",
    "        # for the large values of d_k, the dot products grow large in magnitude, \n",
    "        # pushing the softmax function into regions where it has extremely small gradients\n",
    "        # to counteract this effect, scaled the dot products by 1/sqrt(d_k)\n",
    "        # to illustrate why the dot products get large,\n",
    "        # check the function 'check_dotproduct_dist'\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -np.inf)\n",
    "        \n",
    "        attn = self.softmax(attn)  # (B, T_q, T_k) --> (B, T_q, T_k)\n",
    "        output = torch.bmm(attn, v)  # (B, T_q, T_k) * (B, T_v, d_v) --> (B, T_q, d_v), make sure that T_k == T_v\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex) translation: \n",
    "# q - previous decoder\n",
    "# k, v - encoder output\n",
    "batch = 6\n",
    "T_q = 1\n",
    "T_k, T_v = (7, 7)\n",
    "d_k = 10\n",
    "d_v = 12\n",
    "q, k, v = torch.randn((batch, T_q, d_k)), torch.randn((batch, T_k, d_k)), torch.randn((batch, T_v, d_v))\n",
    "attention = ScaledDotProductAttention(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 1, 12]), torch.Size([6, 1, 7]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, attn = attention(q, k, v)\n",
    "output.size(), attn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dotproduct_dist(d_k, sampling_size=1, seq_len=1, threshold=1e-10):\n",
    "    \"\"\"\n",
    "    to check \"https://arxiv.org/abs/1706.03762\" Paper page 4, annotation 4\n",
    "    -------------------------------\n",
    "    To illustrate why the dot products get large, \n",
    "    assume that the components of q and k are independent random variables \n",
    "    with mean 0 and variance 1.\n",
    "    Then their dot product has mean 0 and variance d_k\n",
    "    \"\"\"\n",
    "    def cal_grad(attn):\n",
    "        y = torch.softmax(attn, dim=2)\n",
    "        return y * (1-y)\n",
    "    \n",
    "    q = nn.init.normal_(torch.rand((sampling_size, seq_len, d_k)), mean=0, std=1)\n",
    "    k = nn.init.normal_(torch.rand((sampling_size, seq_len, d_k)), mean=0, std=1)\n",
    "    attn = torch.bmm(q, k.transpose(1, 2))\n",
    "    print('size of vector d_k is {}, sampling result, dot product distribution has \\n - mean: {:.4f}, \\n - var: {:.4f}'.\\\n",
    "          format(d_k, attn.mean().item(), attn.var().item()))\n",
    "    grad = cal_grad(attn)\n",
    "    print( \"count of gradients that smaller than threshod({}) is {}, {:.4f}%\".format(\n",
    "        threshold, grad.le(threshold).sum(), grad.le(threshold).sum().item()/grad.view(-1).size(0)*100 ) )\n",
    "    attn2 = attn / torch.sqrt(torch.as_tensor(d_k).float())\n",
    "    grad2 = cal_grad(attn2)\n",
    "    print( \"after divide by sqrt(d_k), count of gradients that smaller than threshod({}) is {}, {:.4f}% \\n\".format(\n",
    "        threshold, grad2.le(threshold).sum(), grad2.le(threshold).sum().item()/grad2.view(-1).size(0)*100 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** notice that the gradient of softmax is y(1-y) ***\n",
      "size of vector d_k is 10, sampling result, dot product distribution has \n",
      " - mean: -0.0027, \n",
      " - var: 10.0053\n",
      "count of gradients that smaller than threshod(1e-10) is 176, 0.0070%\n",
      "after divide by sqrt(d_k), count of gradients that smaller than threshod(1e-10) is 0, 0.0000% \n",
      "\n",
      "size of vector d_k is 100, sampling result, dot product distribution has \n",
      " - mean: -0.0039, \n",
      " - var: 100.0387\n",
      "count of gradients that smaller than threshod(1e-10) is 402517, 16.1007%\n",
      "after divide by sqrt(d_k), count of gradients that smaller than threshod(1e-10) is 0, 0.0000% \n",
      "\n",
      "size of vector d_k is 1000, sampling result, dot product distribution has \n",
      " - mean: -0.0409, \n",
      " - var: 998.2934\n",
      "count of gradients that smaller than threshod(1e-10) is 1735779, 69.4312%\n",
      "after divide by sqrt(d_k), count of gradients that smaller than threshod(1e-10) is 0, 0.0000% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"*** notice that the gradient of softmax is y(1-y) ***\")\n",
    "for d_k in [10, 100, 1000]:\n",
    "    check_dotproduct_dist(d_k, sampling_size=100000, seq_len=5, threshold=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(XavierLinear, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.linear(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head Attention\"\"\"\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, drop_rate=0.1):\n",
    "        \"\"\"\n",
    "        paper setting: n_head = 8, d_k = d_v = d_model / n_head = 64\n",
    "        Multi-head attention allows the model to jointly attend to information from \n",
    "        different representation subspaces at different positions.\n",
    "        with a single attention head, averaging inhibits this.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.linear_q = XavierLinear(d_model, d_k)\n",
    "        self.linear_k = XavierLinear(d_model, d_k)\n",
    "        self.linear_v = XavierLinear(d_model, d_v)\n",
    "        self.linear_o = XavierLinear(n_head*d_v, d_model)\n",
    "        self.attention = ScaledDotProductAttention(d_k)\n",
    "        self.drop_out = nn.Dropout(drop_rate)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        * q: (B, T_q, d_model)\n",
    "        * k: (B, T_k, d_model)\n",
    "        * v: (B, T_v, d_model)\n",
    "        ---------------------\n",
    "        Outputs:\n",
    "        * output: (B, T_q, d_model)\n",
    "        * attn: (n_head * B, T_q, T_k)\n",
    "        \"\"\"\n",
    "        n_head, d_model, d_k, d_v = self.n_head, self.d_model, self.d_k, self.d_v\n",
    "        \n",
    "        # repeat to compute n_heads\n",
    "        n_qs = q.repeat(n_head, 1, 1)  # (n_head * B, T_q, d_model)\n",
    "        n_ks = k.repeat(n_head, 1, 1)  # (n_head * B, T_k, d_model)\n",
    "        n_vs = v.repeat(n_head, 1, 1)  # (n_head * B, T_v, d_model)\n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1, 1)\n",
    "        \n",
    "        # through linear layer: \n",
    "        lin_qs = self.linear_q(n_qs)  # (n_head * B, T_q, d_model) --> (n_head * B, T_q, d_k) \n",
    "        lin_ks = self.linear_q(n_ks)  # (n_head * B, T_k, d_model) --> (n_head * B, T_k, d_k) \n",
    "        lin_vs = self.linear_q(n_vs)  # (n_head * B, T_v, d_model) --> (n_head * B, T_v, d_v)\n",
    "        \n",
    "        # attention: Scaled Dot-Product Attention\n",
    "        ## heads: (n_head * B, T_q, d_v)\n",
    "        ## attn: (n_head * B, T_q, T_k)\n",
    "        heads, attn = self.attention(q=lin_qs, k=lin_ks, v=lin_vs, mask=mask)\n",
    "        \n",
    "        # concat\n",
    "        heads_cat = torch.cat(list(heads.chunk(n_head, dim=0)), dim=-1)  # (n_head * B, T_q, d_v) --> (B, T_q, n_head * d_v)\n",
    "        output = self.linear_o(heads_cat)  # (B, T_q, n_head * d_v) --> (B, T_q, d_model)\n",
    "        output = self.drop_out(output)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 1, 512]), torch.Size([6, 7, 512]), torch.Size([6, 7, 512]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = 6\n",
    "T_q = 1\n",
    "T_k, T_v = (7, 7)\n",
    "n_head = 8\n",
    "d_model = 64*n_head\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "q, k, v = torch.randn((batch, T_q, d_model)), torch.randn((batch, T_k, d_model)), torch.randn((batch, T_v, d_model))\n",
    "q.size(), k.size(), v.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of MultiHeadAttention(\n",
       "  (linear_q): XavierLinear(\n",
       "    (linear): Linear(in_features=512, out_features=64, bias=True)\n",
       "  )\n",
       "  (linear_k): XavierLinear(\n",
       "    (linear): Linear(in_features=512, out_features=64, bias=True)\n",
       "  )\n",
       "  (linear_v): XavierLinear(\n",
       "    (linear): Linear(in_features=512, out_features=64, bias=True)\n",
       "  )\n",
       "  (linear_o): XavierLinear(\n",
       "    (linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (attention): ScaledDotProductAttention(\n",
       "    (softmax): Softmax()\n",
       "  )\n",
       "  (drop_out): Dropout(p=0.1)\n",
       ")>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiheadattention = MultiHeadAttention(n_head, d_model, d_k, d_v)\n",
    "o, attn = multiheadattention(q, k, v, mask=None)\n",
    "multiheadattention.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 1, 512]), torch.Size([48, 1, 7]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.size(), attn.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 Application\n",
    "\n",
    "### Encoder-Decoder attention\n",
    "\n",
    "* queries: the previous decoder layer\n",
    "* keys & values: output of the encoder\n",
    "\n",
    "### Encoder + self-attention\n",
    "\n",
    "* queries, keys, values come form the output of the previous layer in the encoder\n",
    "\n",
    "### Decoder + self-attention\n",
    "\n",
    "* need to prevent leftward information flow in the decoder to preserve the auto-regressive property. \n",
    "* implement by masking out (-inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Position-wise Feed-Forward Networks\n",
    "\n",
    "$$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "$$\\begin{aligned} W_1 &\\in \\Bbb{R}^{d_{model} \\times d_f} \\\\ \n",
    "b_1 &\\in \\Bbb{R}^{d_f} \\\\\n",
    "W_2 &\\in \\Bbb{R}^{d_f \\times d_{model}} \\\\ \n",
    "b_2 &\\in \\Bbb{R}^{d_{model}} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "same as $FFN = Linear(ReLU(Linear(x)) = Conv1d(ReLU(Conv1d))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Networks\"\"\"\n",
    "    def __init__(self, d_model, d_f, drop_rate=0.1, use_conv=False):\n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Conv1d(d_model, d_f, kernel_size=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(d_f, d_model, kernel_size=1)\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(d_model, d_f),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(d_f, d_model)\n",
    "            )\n",
    "        self.drop_out = nn.Dropout(drop_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        x: (B, T, d_model)\n",
    "        -----------------------\n",
    "        Ouputs:\n",
    "        output: (B, T, d_model)\n",
    "        \"\"\"\n",
    "        if self.use_conv:\n",
    "            x = x.transpose(1, 2)  # (B, T, d_model) --> (B, d_model, T), reshape like (batch, channel, dim)\n",
    "            output = self.fc(x).transpose(1, 2)  # (B, d_model, T) --> (B, T, d_model)\n",
    "        else:\n",
    "            output = self.fc(x)\n",
    "            \n",
    "        output = self.drop_out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_f = d_model*4\n",
    "PWFFN = PositionWiseFFN(d_model, d_f, use_conv=False)\n",
    "PWFFN(q).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PWFFN = PositionWiseFFN(d_model, d_f, use_conv=True)\n",
    "PWFFN(q).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned} PE_{(pos, 2i)} &= sin(pos/10000^{2i / d_{model}}) \\\\\n",
    "PE_{(pos, 2i+1)} &= cos(pos/10000^{2i / d_{model}}) \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional Encoding\"\"\"\n",
    "    def __init__(self, n_pos, d_model, pad_idx=None):\n",
    "        \"\"\"\n",
    "        n_pos = max sequence length + 1\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.n_pos = n_pos\n",
    "        self.d_model = d_model\n",
    "        self.pad_idx = pad_idx\n",
    "        self.pe_table = np.array(self.get_pe_table())\n",
    "        self.pe_table[:, 0::2] = np.sin(self.pe_table[:, 0::2])\n",
    "        self.pe_table[:, 1::2] = np.cos(self.pe_table[:, 1::2])\n",
    "        if pad_idx is not None:\n",
    "            # zero vector for padding dimension\n",
    "            self.pe_table[pad_idx] = 0.\n",
    "            \n",
    "        self.pe = nn.Embedding.from_pretrained(torch.FloatTensor(self.pe_table), freeze=True)\n",
    "        \n",
    "    def cal_angle(self, pos, hid_idx):\n",
    "        return pos / (10000 ** ((2*(hid_idx // 2) / self.d_model)) )\n",
    "    \n",
    "    def get_pe_table(self):\n",
    "        return [[self.cal_angle(pos, i) for i in range(self.d_model)] for pos in range(self.n_pos)]         \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.pe(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = 3\n",
    "vocab_len = 10\n",
    "pos_layer = PositionalEncoding(n_pos+1, d_model, pad_idx=0)  # n_pos + 1 for pad idx\n",
    "embed_layer = nn.Embedding(vocab_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 512]), torch.Size([4, 3, 512]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.LongTensor(np.array([[6, 5, 3], [1, 3, 0], [5, 3, 6], [3, 6, 2]]))\n",
    "po_x = torch.LongTensor(np.array([[1, 2, 3], [1, 2, 0], [1, 2, 3], [1, 2, 3]]))\n",
    "embed_layer(x).size(), pos_layer(po_x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = embed_layer(x) + pos_layer(po_x)\n",
    "inputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encode_Layer(nn.Module):\n",
    "    \"\"\"encode layer\"\"\"\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, d_f, drop_rate=0.1, use_conv=False):\n",
    "        super(Encode_Layer, self).__init__()\n",
    "        self.selfattn = MultiHeadAttention(n_head, d_model, d_k, d_v, drop_rate=drop_rate)\n",
    "        self.pwffn = PositionWiseFFN(d_model, d_f, drop_rate=drop_rate, use_conv=use_conv)\n",
    "        self.norm_selfattn = nn.LayerNorm(d_model)\n",
    "        self.norm_pwffn = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, enc_input, enc_mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        * enc_input: (B, T, d_model)\n",
    "        * enc_mask: (B, T, T)\n",
    "        -------------------------------------\n",
    "        Outputs:\n",
    "        * enc_output: (B, T, d_model)\n",
    "        * enc_attn: (n_head * B, T, T)\n",
    "        \"\"\"\n",
    "        # Layer: Multi-Head Attention + Add & Norm\n",
    "        # encode self-attention\n",
    "        enc_output, enc_attn = self.selfattn(enc_input, enc_input, enc_input, mask=enc_mask)\n",
    "        enc_output = self.norm_selfattn(enc_input + enc_output)\n",
    "        \n",
    "        # Layer: PositionWiseFFN + Add & Norm\n",
    "        pw_output = self.pwffn(enc_output)\n",
    "        enc_output = self.norm_pwffn(enc_output + pw_output)\n",
    "        \n",
    "        return enc_output, enc_attn\n",
    "\n",
    "\n",
    "class Decode_Layer(nn.Module):\n",
    "    \"\"\"decode layer\"\"\"\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, d_f, drop_rate=0.1, use_conv=False):\n",
    "        super(Decode_Layer, self).__init__()\n",
    "        self.selfattn_masked = MultiHeadAttention(n_head, d_model, d_k, d_v, drop_rate=drop_rate)\n",
    "        self.dec_enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, drop_rate=drop_rate)\n",
    "        self.pwffn = PositionWiseFFN(d_model, d_f, drop_rate=drop_rate, use_conv=use_conv)\n",
    "        self.norm_selfattn_masked = nn.LayerNorm(d_model)\n",
    "        self.norm_dec_enc_attn = nn.LayerNorm(d_model)\n",
    "        self.norm_pwffn = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, dec_input, enc_output, dec_self_mask=None, dec_enc_mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        * dec_input: (B, T_q, d_model)\n",
    "        * enc_input: (B, T, d_model)\n",
    "        * dec_self_mask: (B, T_q, T_q)\n",
    "        * dec_enc_mask: (B, T_q, T)\n",
    "        -------------------------------------\n",
    "        Outputs:\n",
    "        * dec_output: (B, T_q, d_model)\n",
    "        * dec_self_attn: (n_head * B, T_q, T_q)\n",
    "        * dec_enc_attn: (n_head * B, T_q, T)\n",
    "        \"\"\"\n",
    "        # Layer: Multi-Head Attention + Add & Norm\n",
    "        # decode self-attention\n",
    "        dec_self_output, dec_self_attn = self.selfattn_masked(dec_input, dec_input, dec_input, \n",
    "                                                              mask=dec_self_mask)\n",
    "        dec_self_output = self.norm_selfattn_masked(dec_input + dec_self_output)\n",
    "        \n",
    "        # Layer: Multi-Head Attention + Add & Norm\n",
    "        # decode output(queries) + encode output(keys, values)\n",
    "        dec_output, dec_enc_attn = self.dec_enc_attn(dec_self_output, enc_output, enc_output, \n",
    "                                                     mask=dec_enc_mask)\n",
    "        dec_output = self.norm_dec_enc_attn(dec_self_output + dec_output)\n",
    "        \n",
    "        # Layer: PositionWiseFFN + Add & Norm\n",
    "        pw_output = self.pwffn(dec_output)\n",
    "        dec_output = self.norm_pwffn(dec_output + pw_output)\n",
    "        \n",
    "        return dec_output, dec_self_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 512]), torch.Size([4, 4, 512]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_n_pos = 4  # equal to max_seq_len\n",
    "t = torch.LongTensor(np.array([[1, 0, 0, 0], [5, 7, 9, 2], [3, 7, 0, 0], [2, 9, 4, 0]]))\n",
    "po_t = torch.LongTensor(np.array([[1, 0, 0, 0], [1, 2, 3, 4], [1, 2, 0, 0], [1, 2, 3, 0]]))\n",
    "pos_layer = PositionalEncoding(target_n_pos+1, d_model, pad_idx=0)  # n_pos + 1 for pad idx\n",
    "embed_layer = nn.Embedding(10, d_model)\n",
    "embed_layer(t).size(), pos_layer(po_t).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_inputs = embed_layer(t) + pos_layer(po_t)\n",
    "target_inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_layer = Encode_Layer(n_head, d_model, d_k, d_v, d_f)\n",
    "dec_layer = Decode_Layer(n_head, d_model, d_k, d_v, d_f)\n",
    "enc_output, enc_attn = enc_layer.forward(inputs)\n",
    "dec_output, dec_self_attn, dec_enc_attn = dec_layer.forward(target_inputs, enc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 512]), torch.Size([32, 3, 3]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_output.size(), enc_attn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 512]), torch.Size([32, 4, 4]), torch.Size([32, 4, 3]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_output.size(), dec_self_attn.size(), dec_enc_attn.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models: Encoder & Decoder - Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding_mask(q, k=None, pad_idx=0, mode='attn'):\n",
    "    \"\"\"\n",
    "    mode: attn\n",
    "    > mask out for pad in attention with queries & keys sequences\n",
    "    > return shape: (B, T_q, T_k)\n",
    "    mode: subseq\n",
    "    > mask out next tokens to preserve 'auto-regressive property'\n",
    "    > return shape: (B, T_q, T_q)\n",
    "    \"\"\"\n",
    "    B, q_len = q.size()\n",
    "    if mode == 'attn':\n",
    "        assert k is not None, \"must have key sequences\"\n",
    "        padding_mask = k.eq(pad_idx)\n",
    "        padding_mask = padding_mask.unsqueeze(1).expand(B, q_len, -1)\n",
    "        return padding_mask\n",
    "    elif mode =='subseq':\n",
    "        assert k is None, \"don't need key sequences\"\n",
    "        subseq_mask = torch.triu(torch.ones((q_len, q_len), device=q.device, dtype=torch.uint8), \n",
    "                                 diagonal=1)\n",
    "        subseq_mask = subseq_mask.unsqueeze(0).expand(B, -1, -1)\n",
    "        return subseq_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_len, max_seq_len, n_layer, n_head, d_model, d_k, d_v, d_f, \n",
    "                 pad_idx=0, drop_rate=0.1, use_conv=False, return_attn=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.return_attn = return_attn\n",
    "        self.embed_layer = nn.Embedding(vocab_len, d_model, padding_idx=pad_idx)\n",
    "        self.pos_layer = PositionalEncoding(max_seq_len+1, d_model, pad_idx)\n",
    "        self.layers = nn.ModuleList([Encode_Layer(n_head, d_model, d_k, d_v, d_f, \n",
    "                                                  drop_rate=drop_rate, \n",
    "                                                  use_conv=use_conv) \\\n",
    "                                     for i in range(n_layer)])\n",
    "        \n",
    "    def forward(self, enc, enc_pos):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        * enc: (B, T)\n",
    "        * enc_pos: (B, T)\n",
    "        -------------------------------------\n",
    "        Outputs:\n",
    "        * enc_output: (B, T, d_model)\n",
    "        * self_attns: (n_layer, n_head*B, T, T)\n",
    "        \"\"\"\n",
    "        self_attns = []  # (n_layer, n_head*B, T, T)\n",
    "        # self attention padding mask: (B, T, T)\n",
    "        attn_mask = get_padding_mask(q=enc, k=enc, pad_idx=self.pad_idx, mode='attn')\n",
    "        \n",
    "        # embedding + position encoding: (B, T) --> (B, T, d_model)\n",
    "        enc_output = self.embed_layer(enc) + self.pos_layer(enc_pos)\n",
    "        \n",
    "        # forward encode layer\n",
    "        for enc_layer in self.layers:\n",
    "            enc_output, enc_self_attn = enc_layer(enc_input=enc_output, enc_mask=attn_mask)\n",
    "            if self.return_attn:\n",
    "                self_attns.append(enc_self_attn)\n",
    "        \n",
    "        if self.return_attn:\n",
    "            return enc_output, self_attns\n",
    "        return enc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_len, max_seq_len, n_layer, n_head, d_model, d_k, d_v, d_f, \n",
    "                 pad_idx=0, drop_rate=0.1, use_conv=False, return_attn=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.return_attn = return_attn\n",
    "        self.embed_layer = nn.Embedding(vocab_len, d_model, padding_idx=pad_idx)\n",
    "        self.pos_layer = PositionalEncoding(max_seq_len+1, d_model, pad_idx)\n",
    "        self.layers = nn.ModuleList([Decode_Layer(n_head, d_model, d_k, d_v, d_f, \n",
    "                                                  drop_rate=drop_rate, \n",
    "                                                  use_conv=use_conv) \\\n",
    "                                     for i in range(n_layer)])\n",
    "        \n",
    "    def forward(self, dec, dec_pos, enc, enc_output):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        * dec: (B, T_q)\n",
    "        * dec_pos: (B, T_q)\n",
    "        * enc: (B, T)\n",
    "        * enc_output: (B, T, d_model)\n",
    "        -------------------------------------\n",
    "        Outputs:\n",
    "        * dec_output: (B, T_q, d_model)\n",
    "        * self_attns: (n_layer, n_head*B, T_q, T_q)\n",
    "        * dec_enc_attns: (n_layer, n_haed*B, T_q, T)\n",
    "        \"\"\"\n",
    "        self_attns = []  # (n_layer, n_head*B, T_q, T_q)\n",
    "        dec_enc_attns = []  # (n_layer, n_head*B, T_q, T)\n",
    "        \n",
    "        # self attention padding mask: (B, T_q, T)\n",
    "        attn_mask = get_padding_mask(q=dec, k=dec, pad_idx=self.pad_idx, mode='attn')\n",
    "        subseq_mask = get_padding_mask(q=dec, mode='subseq')\n",
    "        self_attn_mask = (attn_mask + subseq_mask).gt(0)\n",
    "        # enc_dec attention padding mask\n",
    "        dec_enc_attn_mask = get_padding_mask(q=dec, k=enc, pad_idx=self.pad_idx, mode='attn')\n",
    "        \n",
    "        # embedding + position encoding: (B, T) --> (B, T, d_model)\n",
    "        dec_output = self.embed_layer(dec) + self.pos_layer(dec_pos)\n",
    "        \n",
    "        # forward decode layer\n",
    "        for dec_layer in self.layers:\n",
    "            dec_output, dec_self_attn, dec_enc_attn = dec_layer(dec_input=dec_output, \n",
    "                                                                enc_output=enc_output, \n",
    "                                                                dec_self_mask=self_attn_mask, \n",
    "                                                                dec_enc_mask=dec_enc_attn_mask)\n",
    "            if self.return_attn:\n",
    "                self_attns.append(dec_self_attn)\n",
    "                dec_enc_attns.append(dec_enc_attn)\n",
    "        \n",
    "        if self.return_attn:\n",
    "            return dec_output, self_attns, dec_enc_attns\n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = 3\n",
    "encoder = Encoder(vocab_len, 3, n_layer, n_head, d_model, d_k, d_v, d_f)\n",
    "decoder = Decoder(vocab_len, 4, n_layer, n_head, d_model, d_k, d_v, d_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_output, enc_self_attns = encoder(x, po_x)\n",
    "dec_output, dec_self_attns, dec_enc_attns = decoder.forward(t, po_t, x, enc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 512]), torch.Size([4, 4, 512]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_output.size(), dec_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, enc_vocab_len, enc_max_seq_len, dec_vocab_len, dec_max_seq_len, \n",
    "                 n_layer, n_head, d_model, d_k, d_v, d_f, \n",
    "                 pad_idx=0, drop_rate=0.1, use_conv=False, return_attn=True,\n",
    "                 linear_weight_share=True, embed_weight_share=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.return_attn = return_attn\n",
    "        self.encoder = Encoder(enc_vocab_len, enc_max_seq_len, n_layer, n_head, \n",
    "                               d_model, d_k, d_v, d_f, \n",
    "                               pad_idx=pad_idx, \n",
    "                               drop_rate=drop_rate, \n",
    "                               use_conv=use_conv, \n",
    "                               return_attn=return_attn)\n",
    "        self.decoder = Decoder(dec_vocab_len, dec_max_seq_len, n_layer, n_head, \n",
    "                               d_model, d_k, d_v, d_f,\n",
    "                               pad_idx=pad_idx, \n",
    "                               drop_rate=drop_rate, \n",
    "                               use_conv=use_conv, \n",
    "                               return_attn=return_attn)\n",
    "        self.projection = XavierLinear(d_model, dec_vocab_len, bias=False)\n",
    "        if linear_weight_share:\n",
    "            # share the same weight matrix between the decoder embedding layer \n",
    "            # and the pre-softmax linear transformation\n",
    "            self.projection.linear.weight = self.decoder.embed_layer.weight\n",
    "        \n",
    "        if embed_weight_share:\n",
    "            # share the same weight matrix between the decoder embedding layer \n",
    "            # and the encoder embedding layer\n",
    "            assert enc_vocab_len == dec_vocab_len, \"vocab length must be same\"\n",
    "            self.encoder.embed_layer.weight = self.decoder.embed_layer.weight\n",
    "            \n",
    "    def forward(self, enc, enc_pos, dec, dec_pos):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        * enc: (B, T)\n",
    "        * enc_pos: (B, T)\n",
    "        * dec: (B, T_q)\n",
    "        * dec_pos: (B, T_q)\n",
    "        -------------------------------------\n",
    "        Outputs:\n",
    "        * dec_output: (B, T_q, d_model)\n",
    "        * attns_dict:\n",
    "            * enc_self_attns: (n_layer, n_head*B, T, T)\n",
    "            * dec_self_attns: (n_layer, n_head*B, T_q, T_q)\n",
    "            * dec_enc_attns: (n_layer, n_haed*B, T_q, T)\n",
    "        \"\"\"\n",
    "        enc_output, enc_self_attns = self.encoder(enc, enc_pos)\n",
    "        dec_output, dec_self_attns, dec_enc_attns = self.decoder(dec, dec_pos, enc, enc_output)\n",
    "        dec_output = self.projection(dec_output)\n",
    "        attns_dict = {'enc_self_attns': enc_self_attns, \n",
    "                     'dec_self_attns': dec_self_attns,\n",
    "                     'dec_enc_attns': dec_enc_attns}\n",
    "        if self.return_attn:\n",
    "            return dec_output, attns_dict\n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(vocab_len, 3, vocab_len, 4, n_layer, n_head, d_model, d_k, d_v, d_f, \n",
    "                 pad_idx=0, drop_rate=0.1, use_conv=False, return_attn=True,\n",
    "                 linear_weight_share=True, embed_weight_share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_output, attns_dict = model(x, po_x, t, po_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 10])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_output.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
