{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maps input sequences to a sequence of continuous representations $x=(x_1 \\cdots, x_n) \\rightarrow z=(z_1, \\cdots, y_n)$, given $z$ generates an output sequence $y=(y_1, \\cdots, y_n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.modules):\n",
    "    \"\"\"Scaled Dot-Product Attention\"\"\"\n",
    "    def __init__(self, d_k):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        q: d_k\n",
    "        k: d_k\n",
    "        v: d_v\n",
    "        \"\"\"\n",
    "        attn = torch.bmm(q, k.transpose(1, 2)) # (B, 1, d_k) * (B, 1, d_k) -> (B, 1, 1)\n",
    "        attn = attn / torch.sqrt(self.d_k)  \n",
    "        # why doing this? \n",
    "        # for the large values of d_k, the dot products grow large in magnitude, \n",
    "        # pushing the softmax function into regions where it has extremely small gradients\n",
    "        # to counteract this effect, scaled the dot products by 1/sqrt(d_k)\n",
    "        # to illustrate why the dot products get large, check the function 'check_dotproduct_dist'\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dotproduct_dist(d_k, sampling_size=1):\n",
    "    \"\"\"\n",
    "    to check Paper page 4, annotation 4\n",
    "    -------------------------------\n",
    "    To illustrate why the dot products get large, \n",
    "    assume that the components of q and k are independent random variables \n",
    "    with mean 0 and variance 1.\n",
    "    Then their dot product has mean 0 and variance d_k\n",
    "    \"\"\"\n",
    "    temp = []\n",
    "    for i in range(sampling_size):\n",
    "        q = nn.init.normal_(torch.rand((d_k)), mean=0, std=1)\n",
    "        k = nn.init.normal_(torch.rand((d_k)), mean=0, std=1)\n",
    "        attn = torch.dot(q, k)\n",
    "        temp.append(attn.item())\n",
    "    print('size of vector d_k is {}, sampling result, dot product distribution has \\n - mean: {}, \\n - std: {}\\n'.\\\n",
    "          format(d_k, np.mean(temp), np.std(temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vector d_k is 10, sampling result, dot product distribution has \n",
      " - mean: -0.021629880513213576, \n",
      " - std: 3.1659752059733126\n",
      "\n",
      "size of vector d_k is 100, sampling result, dot product distribution has \n",
      " - mean: -0.016482602213025093, \n",
      " - std: 10.009568765651423\n",
      "\n",
      "size of vector d_k is 1000, sampling result, dot product distribution has \n",
      " - mean: 0.04095287548005581, \n",
      " - std: 31.607727087367685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d_k in [10, 100, 1000]:\n",
    "    check_dotproduct_dist(d_k, sampling_size=100000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
