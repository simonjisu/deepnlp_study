{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paper : https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://nlp.seas.harvard.edu/2018/04/03/attention.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maps input sequences to a sequence of continuous representations $x=(x_1 \\cdots, x_n) \\rightarrow z=(z_1, \\cdots, y_n)$, given $z$ generates an output sequence $y=(y_1, \\cdots, y_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled Dot-Product Attention\"\"\"\n",
    "    def __init__(self, d_k, return_attn=True):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.return_attn = return_attn\n",
    "        self.d_k = d_k\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        * q: (B, T_q, d_q), d_q = d_k\n",
    "        * k: (B, T_k, d_k)\n",
    "        * v: (B, T_v, d_v), T_k = T_v\n",
    "        -------------------------------\n",
    "        Outputs:\n",
    "        * output: (B, T_q, d_v)\n",
    "        * probs: (B, T_q, T_k)\n",
    "        \"\"\"\n",
    "        assert q.size(2) == k.size(2), \"d_q = d_k\"\n",
    "        assert k.size(1) == v.size(1), \"T_k = T_v\"\n",
    "        attn = torch.bmm(q, k.transpose(1, 2))  # (B, T_q, d_k) * (B, T_k, d_k) -> (B, T_q, T_k)\n",
    "        attn = attn / np.sqrt(self.d_k)\n",
    "        # why doing this? \n",
    "        # for the large values of d_k, the dot products grow large in magnitude, \n",
    "        # pushing the softmax function into regions where it has extremely small gradients\n",
    "        # to counteract this effect, scaled the dot products by 1/sqrt(d_k)\n",
    "        # to illustrate why the dot products get large,\n",
    "        # check the function 'check_dotproduct_dist'\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -np.inf)\n",
    "        \n",
    "        attn = self.softmax(attn)  # (B, T_q, T_k) --> (B, T_q, T_k)\n",
    "        output = torch.bmm(attn, v)  # (B, T_q, T_k) * (B, T_v, d_v) --> (B, T_q, d_v), make sure that T_k == T_v\n",
    "        if self.return_attn:\n",
    "            return output, attn\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex) translation: \n",
    "# q - previous decoder\n",
    "# k, v - encoder output\n",
    "# mask should be (B, T_q, T_k), will talk about it later\n",
    "batch = 1\n",
    "T_q = 3\n",
    "T_k, T_v = (4, 4)\n",
    "d_k = 10\n",
    "d_v = 12\n",
    "mask = torch.ByteTensor([[[0, 1, 1, 1],\n",
    "                          [0, 0, 1, 1],\n",
    "                          [0, 0, 1, 1]]])\n",
    "q, k, v = torch.randn((batch, T_q, d_k)), torch.randn((batch, T_k, d_k)), torch.randn((batch, T_v, d_v))\n",
    "attention = ScaledDotProductAttention(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 12]), torch.Size([1, 3, 4]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, attn = attention(q, k, v, mask=mask)\n",
    "output.size(), attn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(attn.squeeze(0).numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dotproduct_dist(d_k, sampling_size=1, seq_len=1, threshold=1e-10):\n",
    "    \"\"\"\n",
    "    to check \"https://arxiv.org/abs/1706.03762\" Paper page 4, annotation 4\n",
    "    -------------------------------\n",
    "    To illustrate why the dot products get large, \n",
    "    assume that the components of q and k are independent random variables \n",
    "    with mean 0 and variance 1.\n",
    "    Then their dot product has mean 0 and variance d_k\n",
    "    \"\"\"\n",
    "    def cal_grad(attn):\n",
    "        y = torch.softmax(attn, dim=2)\n",
    "        return y * (1-y)\n",
    "    \n",
    "    q = nn.init.normal_(torch.rand((sampling_size, seq_len, d_k)), mean=0, std=1)\n",
    "    k = nn.init.normal_(torch.rand((sampling_size, seq_len, d_k)), mean=0, std=1)\n",
    "    attn = torch.bmm(q, k.transpose(1, 2))\n",
    "    print('size of vector d_k is {}, sampling result, dot product distribution has \\n - mean: {:.4f}, \\n - var: {:.4f}'.\\\n",
    "          format(d_k, attn.mean().item(), attn.var().item()))\n",
    "    grad = cal_grad(attn)\n",
    "    print( \"count of gradients that smaller than threshod({}) is {}, {:.4f}%\".format(\n",
    "        threshold, grad.le(threshold).sum(), grad.le(threshold).sum().item()/grad.view(-1).size(0)*100 ) )\n",
    "    attn2 = attn / torch.sqrt(torch.as_tensor(d_k).float())\n",
    "    grad2 = cal_grad(attn2)\n",
    "    print( \"after divide by sqrt(d_k), count of gradients that smaller than threshod({}) is {}, {:.4f}% \\n\".format(\n",
    "        threshold, grad2.le(threshold).sum(), grad2.le(threshold).sum().item()/grad2.view(-1).size(0)*100 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** notice that the gradient of softmax is y(1-y) ***\n",
      "size of vector d_k is 10, sampling result, dot product distribution has \n",
      " - mean: 0.0003, \n",
      " - var: 10.0183\n",
      "count of gradients that smaller than threshod(1e-10) is 199, 0.0080%\n",
      "after divide by sqrt(d_k), count of gradients that smaller than threshod(1e-10) is 0, 0.0000% \n",
      "\n",
      "size of vector d_k is 100, sampling result, dot product distribution has \n",
      " - mean: -0.0067, \n",
      " - var: 100.0631\n",
      "count of gradients that smaller than threshod(1e-10) is 402257, 16.0903%\n",
      "after divide by sqrt(d_k), count of gradients that smaller than threshod(1e-10) is 0, 0.0000% \n",
      "\n",
      "size of vector d_k is 1000, sampling result, dot product distribution has \n",
      " - mean: -0.0332, \n",
      " - var: 1000.6940\n",
      "count of gradients that smaller than threshod(1e-10) is 1738234, 69.5294%\n",
      "after divide by sqrt(d_k), count of gradients that smaller than threshod(1e-10) is 0, 0.0000% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"*** notice that the gradient of softmax is y(1-y) ***\")\n",
    "for d_k in [10, 100, 1000]:\n",
    "    check_dotproduct_dist(d_k, sampling_size=100000, seq_len=5, threshold=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(XavierLinear, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.linear(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head Attention\"\"\"\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, drop_rate=0.1, return_attn=True):\n",
    "        \"\"\"\n",
    "        paper setting: n_head = 8, d_k = d_v = d_model / n_head = 64\n",
    "        Multi-head attention allows the model to jointly attend to information from \n",
    "        different representation subspaces at different positions.\n",
    "        with a single attention head, averaging inhibits this.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.return_attn = return_attn\n",
    "        self.linear_q = XavierLinear(d_model, n_head*d_k)\n",
    "        self.linear_k = XavierLinear(d_model, n_head*d_k)\n",
    "        self.linear_v = XavierLinear(d_model, n_head*d_v)\n",
    "        self.linear_o = XavierLinear(n_head*d_v, d_model)\n",
    "        self.attention = ScaledDotProductAttention(d_k, return_attn=return_attn)\n",
    "        self.drop_out = nn.Dropout(drop_rate)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        * q: (B, T_q, d_model)\n",
    "        * k: (B, T_k, d_model)\n",
    "        * v: (B, T_v, d_model)\n",
    "        * mask: (B, T_q, T_k)\n",
    "        ---------------------\n",
    "        Outputs:\n",
    "        * output: (B, T_q, d_model)\n",
    "        * attn: (n_head * B, T_q, T_k)\n",
    "        \"\"\"\n",
    "        n_head, d_model, d_k, d_v = self.n_head, self.d_model, self.d_k, self.d_v\n",
    "        B, T_q, _ = q.size()\n",
    "        B, T_k, _ = k.size()\n",
    "        B, T_v, _ = v.size()\n",
    "        # through linear layer: \n",
    "        # lin_qs : (B, T_q, d_model) --> (B, T_q, n_head * d_k) --> (n_head * B, T_q, d_k)\n",
    "        # lin_ks : (B, T_k, d_model) --> (B, T_k, n_head * d_k) --> (n_head * B, T_k, d_k) \n",
    "        # lin_vs : (B, T_v, d_model) --> (B, T_v, n_head * d_v) --> (n_head * B, T_v, d_v)\n",
    "        lin_qs = self.linear_q(q).view(B, T_q, n_head, d_k)  \n",
    "        lin_ks = self.linear_k(k).view(B, T_k, n_head, d_k)  \n",
    "        lin_vs = self.linear_v(v).view(B, T_v, n_head, d_v)  \n",
    "        lin_qs = lin_qs.permute(2, 0, 1, 3).contiguous().view(-1, T_q, d_k)\n",
    "        lin_ks = lin_ks.permute(2, 0, 1, 3).contiguous().view(-1, T_k, d_k)\n",
    "        lin_vs = lin_vs.permute(2, 0, 1, 3).contiguous().view(-1, T_v, d_v)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1, 1)\n",
    "\n",
    "        # attention: Scaled Dot-Product Attention\n",
    "        ## heads: (n_head * B, T_q, d_v)\n",
    "        ## attn: (n_head * B, T_q, T_k)\n",
    "        if self.return_attn:\n",
    "            heads, attn = self.attention(q=lin_qs, k=lin_ks, v=lin_vs, mask=mask)\n",
    "        else:\n",
    "            heads = self.attention(q=lin_qs, k=lin_ks, v=lin_vs, mask=mask)\n",
    "        # concat\n",
    "        # (n_head * B, T_q, d_v) --> (B, T_q, n_head * d_v)\n",
    "        heads_cat = torch.cat(list(heads.chunk(n_head, dim=0)), dim=-1)  \n",
    "        output = self.linear_o(heads_cat)  # (B, T_q, n_head * d_v) --> (B, T_q, d_model)\n",
    "        output = self.drop_out(output)\n",
    "        if self.return_attn:\n",
    "            return output, attn\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 24]), torch.Size([1, 5, 24]), torch.Size([1, 5, 24]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = 1\n",
    "T_q = 2\n",
    "T_k, T_v = (5, 5)\n",
    "n_head = 8\n",
    "d_model = 3*n_head\n",
    "d_k = 5\n",
    "d_v = 5\n",
    "mask = torch.ByteTensor([[[0, 1, 1, 1, 1],\n",
    "                          [0, 0, 1, 1, 1]]])\n",
    "q, k, v = torch.randn((batch, T_q, d_model)), torch.randn((batch, T_k, d_model)), torch.randn((batch, T_v, d_model))\n",
    "q.size(), k.size(), v.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 24]), torch.Size([8, 2, 5]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiheadattention = MultiHeadAttention(n_head, d_model, d_k, d_v)\n",
    "o, attn = multiheadattention(q, k, v, mask=mask)\n",
    "o.size(), attn.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 Application\n",
    "\n",
    "### Encoder-Decoder attention\n",
    "\n",
    "* queries: the previous decoder layer\n",
    "* keys & values: output of the encoder\n",
    "\n",
    "### Encoder + self-attention\n",
    "\n",
    "* queries, keys, values come form the output of the previous layer in the encoder\n",
    "\n",
    "### Decoder + self-attention\n",
    "\n",
    "* need to prevent leftward information flow in the decoder to preserve the auto-regressive property. \n",
    "* implement by masking out (-inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Position-wise Feed-Forward Networks\n",
    "\n",
    "$$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "$$\\begin{aligned} W_1 &\\in \\Bbb{R}^{d_{model} \\times d_f} \\\\ \n",
    "b_1 &\\in \\Bbb{R}^{d_f} \\\\\n",
    "W_2 &\\in \\Bbb{R}^{d_f \\times d_{model}} \\\\ \n",
    "b_2 &\\in \\Bbb{R}^{d_{model}} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "same as $FFN = Linear(ReLU(Linear(x)) = Conv1d(ReLU(Conv1d))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Networks\"\"\"\n",
    "    def __init__(self, d_model, d_f, drop_rate=0.1, use_conv=False):\n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Conv1d(d_model, d_f, kernel_size=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(d_f, d_model, kernel_size=1)\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(d_model, d_f),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(d_f, d_model)\n",
    "            )\n",
    "        self.drop_out = nn.Dropout(drop_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        x: (B, T, d_model)\n",
    "        -----------------------\n",
    "        Ouputs:\n",
    "        output: (B, T, d_model)\n",
    "        \"\"\"\n",
    "        if self.use_conv:\n",
    "            x = x.transpose(1, 2)  # (B, T, d_model) --> (B, d_model, T), reshape like (batch, channel, dim)\n",
    "            output = self.fc(x).transpose(1, 2)  # (B, d_model, T) --> (B, T, d_model)\n",
    "        else:\n",
    "            output = self.fc(x)\n",
    "            \n",
    "        output = self.drop_out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_f = d_model*4\n",
    "PWFFN = PositionWiseFFN(d_model, d_f, use_conv=False)\n",
    "PWFFN(q).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PWFFN = PositionWiseFFN(d_model, d_f, use_conv=True)\n",
    "PWFFN(q).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Positional Encoding & Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned} PE_{(pos, 2i)} &= sin(pos/10000^{2i / d_{model}}) \\\\\n",
    "PE_{(pos, 2i+1)} &= cos(pos/10000^{2i / d_{model}}) \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional Encoding\"\"\"\n",
    "    def __init__(self, n_pos, d_model, pad_idx=0):\n",
    "        \"\"\"\n",
    "        n_pos = max sequence length + 1\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.n_pos = n_pos\n",
    "        self.d_model = d_model\n",
    "        self.pe_table = np.array(self.get_pe_table())\n",
    "        self.pe_table[:, 0::2] = np.sin(self.pe_table[:, 0::2])\n",
    "        self.pe_table[:, 1::2] = np.cos(self.pe_table[:, 1::2])\n",
    "        self.pe_table[pad_idx, :] = 0  # embed all pad to 0\n",
    "        self.pe = nn.Embedding.from_pretrained(torch.FloatTensor(self.pe_table), freeze=True)\n",
    "        \n",
    "    def cal_angle(self, pos, hid_idx):\n",
    "        return pos / (10000 ** ((2*(hid_idx // 2) / self.d_model)) )\n",
    "    \n",
    "    def get_pe_table(self):\n",
    "        return [[self.cal_angle(pos, i) for i in range(self.d_model)] for pos in range(self.n_pos)]         \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.pe(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_len, d_model, pad_idx=1):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_len, d_model, padding_idx=pad_idx)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * np.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = 3\n",
    "vocab_len = 10\n",
    "pos_layer = PositionalEncoding(n_pos+1, d_model)  # n_pos + 1 for pad idx\n",
    "embed_layer = Embedding(vocab_len, d_model, pad_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 256]), torch.Size([4, 3, 256]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.LongTensor(np.array([[6, 5, 3], [1, 3, 0], [5, 3, 6], [3, 6, 2]]))\n",
    "po_x = torch.LongTensor(np.array([[1, 2, 3], [1, 2, 0], [1, 2, 3], [1, 2, 3]]))\n",
    "embed_layer(x).size(), pos_layer(po_x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = embed_layer(x) + pos_layer(po_x)\n",
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 256]), torch.Size([4, 4, 256]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_n_pos = 4  # equal to max_seq_len\n",
    "t = torch.LongTensor(np.array([[1, 0, 0, 0], [5, 7, 9, 2], [3, 7, 0, 0], [2, 9, 4, 0]]))\n",
    "po_t = torch.LongTensor(np.array([[1, 0, 0, 0], [1, 2, 3, 4], [1, 2, 0, 0], [1, 2, 3, 0]]))\n",
    "pos_layer = PositionalEncoding(target_n_pos+1, d_model)  # n_pos + 1 for pad idx\n",
    "embed_layer = Embedding(vocab_len, d_model)\n",
    "embed_layer(t).size(), pos_layer(po_t).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_inputs = embed_layer(t) + pos_layer(po_t)\n",
    "target_inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding_mask(q, k=None, pad_idx=1, mode='attn'):\n",
    "    \"\"\"\n",
    "    mode: attn\n",
    "    > mask out for pad in attention with queries & keys sequences\n",
    "    > return shape: (B, T_q, T_k)\n",
    "    mode: nonpad\n",
    "    > mask out pad rows in attention\n",
    "    > return shape: (B, T_q, T_k)\n",
    "    mode: subseq\n",
    "    > mask out next tokens to preserve 'auto-regressive property'\n",
    "    > return shape: (B, T_q, T_q)\n",
    "    \"\"\"\n",
    "    B, q_len = q.size()\n",
    "    if mode == 'attn':\n",
    "        assert k is not None, \"must have key sequences\"\n",
    "        padding_mask = k.eq(pad_idx)\n",
    "        padding_mask = padding_mask.unsqueeze(1).expand(B, q_len, -1)\n",
    "        return padding_mask\n",
    "    elif mode == 'nonpad':\n",
    "        # to mask out pad rows\n",
    "        assert k is None, \"don't need key sequences\"\n",
    "        return q.ne(pad_idx).type(torch.float).unsqueeze(-1)\n",
    "    elif mode =='subseq':\n",
    "        assert k is None, \"don't need key sequences\"\n",
    "        subseq_mask = torch.triu(torch.ones((q_len, q_len), device=q.device, dtype=torch.uint8), \n",
    "                                 diagonal=1)\n",
    "        subseq_mask = subseq_mask.unsqueeze(0).expand(B, -1, -1)\n",
    "        return subseq_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = get_padding_mask(q=t, k=t, pad_idx=0, mode='attn')\n",
    "subseq_mask = get_padding_mask(q=t, mode='subseq')\n",
    "self_attn_mask = (attn_mask + subseq_mask).gt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 1, 1],\n",
       "         [0, 1, 1, 1],\n",
       "         [0, 1, 1, 1],\n",
       "         [0, 1, 1, 1]],\n",
       "\n",
       "        [[0, 1, 1, 1],\n",
       "         [0, 0, 1, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 1, 1, 1],\n",
       "         [0, 0, 1, 1],\n",
       "         [0, 0, 1, 1],\n",
       "         [0, 0, 1, 1]],\n",
       "\n",
       "        [[0, 1, 1, 1],\n",
       "         [0, 0, 1, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         [0, 0, 0, 1]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_pad_mask = get_padding_mask(q=t, pad_idx=0, mode='nonpad')\n",
    "non_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiheadattention = MultiHeadAttention(n_head, d_model, d_k, d_v)\n",
    "o, attn = multiheadattention(q=target_inputs, k=target_inputs, v=target_inputs, mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "o *= non_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.4242,  -0.3655,   0.3953,  ...,  -0.1497,   0.4583,  -0.0049],\n",
       "         [ -0.0000,  -0.0000,   0.0000,  ...,  -0.0000,   0.0000,  -0.0000],\n",
       "         [ -0.0000,  -0.0000,   0.0000,  ...,  -0.0000,   0.0000,  -0.0000],\n",
       "         [ -0.0000,  -0.0000,   0.0000,  ...,  -0.0000,   0.0000,  -0.0000]],\n",
       "\n",
       "        [[  2.8123,   0.0000,  -0.0539,  ..., -11.1668,  -4.8402,  23.7759],\n",
       "         [ 15.8007,   8.3555, -21.1437,  ...,  11.6008,  16.7544,  19.9623],\n",
       "         [  7.0687,  -0.0000,   9.5903,  ...,  13.4302,  11.8907,  -9.4380],\n",
       "         [-13.1387,  11.2663,  11.0090,  ..., -35.1497,  -0.0000,  -4.5492]],\n",
       "\n",
       "        [[-44.0997,  13.5027,  11.6549,  ...,   8.9507,  10.0594,  11.9547],\n",
       "         [-11.5073,   7.3753,  -8.2744,  ...,  12.8736,   7.3027,  16.2490],\n",
       "         [ -0.0000,   0.0000,  -0.0000,  ...,   0.0000,   0.0000,  -0.0000],\n",
       "         [ -0.0000,   0.0000,  -0.0000,  ...,   0.0000,   0.0000,  -0.0000]],\n",
       "\n",
       "        [[ -7.1606,   0.0000,  20.6702,  ...,   1.6610, -14.5098,  -8.6210],\n",
       "         [ -0.0000,   4.1631,  18.4352,  ...,  -0.4344,  -0.0000,   9.5706],\n",
       "         [-10.5211,  19.1705,  -1.4648,  ...,  -6.5956,  -0.0000,  29.5595],\n",
       "         [ -0.0000,   0.0000,   0.0000,  ...,  -0.0000,  -0.0000,  -0.0000]]],\n",
       "       grad_fn=<ThMulBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Layer\n",
    "class Encode_Layer(nn.Module):\n",
    "    \"\"\"encode layer\"\"\"\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, d_f, drop_rate=0.1, use_conv=False, return_attn=True):\n",
    "        super(Encode_Layer, self).__init__()\n",
    "        self.return_attn = return_attn\n",
    "        self.n_head = n_head\n",
    "        self.selfattn = MultiHeadAttention(n_head, d_model, d_k, d_v, drop_rate=drop_rate, return_attn=return_attn)\n",
    "        self.pwffn = PositionWiseFFN(d_model, d_f, drop_rate=drop_rate, use_conv=use_conv)\n",
    "        self.norm_selfattn = nn.LayerNorm(d_model)\n",
    "        self.norm_pwffn = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, enc_input, enc_mask=None, non_pad_mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        * enc_input: (B, T, d_model)\n",
    "        * enc_mask: (B, T, T)\n",
    "        * non_pad_mask: (B, T, 1)\n",
    "        -------------------------------------\n",
    "        Outputs:\n",
    "        * enc_output: (B, T, d_model)\n",
    "        * enc_attn: (n_head * B, T, T)\n",
    "        \"\"\"\n",
    "        # Layer: Multi-Head Attention + Add & Norm\n",
    "        # encode self-attention\n",
    "        if self.return_attn:\n",
    "            enc_output, enc_attn = self.selfattn(enc_input, enc_input, enc_input, mask=enc_mask)\n",
    "        else:\n",
    "            enc_output = self.selfattn(enc_input, enc_input, enc_input, mask=enc_mask)\n",
    "        enc_output = self.norm_selfattn(enc_input + enc_output)\n",
    "        enc_output *= non_pad_mask\n",
    "        \n",
    "        # Layer: PositionWiseFFN + Add & Norm\n",
    "        pw_output = self.pwffn(enc_output)\n",
    "        enc_output = self.norm_pwffn(enc_output + pw_output)\n",
    "        enc_output *= non_pad_mask\n",
    "        if self.return_attn:\n",
    "            # attns cat([B, T, T] * n_heads)\n",
    "            enc_attn = torch.cat([attn*non_pad_mask \\\n",
    "                                  for attn in enc_attn.chunk(self.n_head, dim=0)], dim=0)\n",
    "            return enc_output, enc_attn\n",
    "        return enc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode Layer\n",
    "class Decode_Layer(nn.Module):\n",
    "    \"\"\"decode layer\"\"\"\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, d_f, drop_rate=0.1, use_conv=False, return_attn=True):\n",
    "        super(Decode_Layer, self).__init__()\n",
    "        self.return_attn = return_attn\n",
    "        self.n_head = n_head\n",
    "        self.selfattn_masked = MultiHeadAttention(n_head, d_model, d_k, d_v, drop_rate=drop_rate, \n",
    "                                                  return_attn=return_attn)\n",
    "        self.dec_enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, drop_rate=drop_rate, \n",
    "                                               return_attn=return_attn)\n",
    "        self.pwffn = PositionWiseFFN(d_model, d_f, drop_rate=drop_rate, use_conv=use_conv)\n",
    "        self.norm_selfattn_masked = nn.LayerNorm(d_model)\n",
    "        self.norm_dec_enc_attn = nn.LayerNorm(d_model)\n",
    "        self.norm_pwffn = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, dec_input, enc_output, dec_self_mask=None, \n",
    "                dec_enc_mask=None, non_pad_mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        * dec_input: (B, T_q, d_model)\n",
    "        * enc_input: (B, T, d_model)\n",
    "        * dec_self_mask: (B, T_q, T_q)\n",
    "        * dec_enc_mask: (B, T_q, T)\n",
    "        * non_pad_mask: (B, T_q, 1)\n",
    "        -------------------------------------\n",
    "        Outputs:\n",
    "        * dec_output: (B, T_q, d_model)\n",
    "        * dec_self_attn: (n_head * B, T_q, T_q)\n",
    "        * dec_enc_attn: (n_head * B, T_q, T)\n",
    "        \"\"\"\n",
    "        # Layer: Multi-Head Attention + Add & Norm\n",
    "        # decode self-attention\n",
    "        if self.return_attn:\n",
    "            dec_self_output, dec_self_attn = self.selfattn_masked(dec_input, dec_input, dec_input, \n",
    "                                                                  mask=dec_self_mask)\n",
    "        else:\n",
    "            dec_self_output = self.selfattn_masked(dec_input, dec_input, dec_input, \n",
    "                                                   mask=dec_self_mask)\n",
    "        dec_self_output = self.norm_selfattn_masked(dec_input + dec_self_output)\n",
    "        dec_self_output *= non_pad_mask\n",
    "        \n",
    "        # Layer: Multi-Head Attention + Add & Norm\n",
    "        # decode output(queries) + encode output(keys, values)\n",
    "        if self.return_attn:\n",
    "            dec_output, dec_enc_attn = self.dec_enc_attn(dec_self_output, enc_output, enc_output, \n",
    "                                                         mask=dec_enc_mask)\n",
    "        else:\n",
    "             dec_output = self.dec_enc_attn(dec_self_output, enc_output, enc_output, mask=dec_enc_mask)\n",
    "        dec_output = self.norm_dec_enc_attn(dec_self_output + dec_output)\n",
    "        dec_output *= non_pad_mask\n",
    "        \n",
    "        # Layer: PositionWiseFFN + Add & Norm\n",
    "        pw_output = self.pwffn(dec_output)\n",
    "        dec_output = self.norm_pwffn(dec_output + pw_output)\n",
    "        dec_output *= non_pad_mask\n",
    "        if self.return_attn:\n",
    "            dec_self_attn = torch.cat([attn*non_pad_mask \\\n",
    "                                       for attn in dec_self_attn.chunk(self.n_head, dim=0)], dim=0)\n",
    "            dec_enc_attn = torch.cat([attn*non_pad_mask \\\n",
    "                                      for attn in dec_enc_attn.chunk(self.n_head, dim=0)], dim=0)\n",
    "            return dec_output, dec_self_attn, dec_enc_attn\n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_layer = Encode_Layer(n_head, d_model, d_k, d_v, d_f)\n",
    "dec_layer = Decode_Layer(n_head, d_model, d_k, d_v, d_f)\n",
    "\n",
    "enc_mask = get_padding_mask(x, x, pad_idx=0, mode='attn')\n",
    "non_pad_mask = get_padding_mask(x, pad_idx=0, mode='nonpad')\n",
    "enc_output, enc_attn = enc_layer.forward(inputs, enc_mask=enc_mask, non_pad_mask=non_pad_mask)\n",
    "\n",
    "dec_mask = (get_padding_mask(t, t, pad_idx=0, mode='attn') + \\\n",
    "    get_padding_mask(t, pad_idx=0, mode='subseq')).gt(0)\n",
    "dec_enc_mask = get_padding_mask(t, x, pad_idx=0, mode='attn')\n",
    "non_pad_mask = get_padding_mask(t, pad_idx=0, mode='nonpad')\n",
    "\n",
    "dec_output, dec_self_attn, dec_enc_attn = dec_layer.forward(target_inputs, \n",
    "                                                            enc_output, \n",
    "                                                            dec_self_mask=dec_mask, \n",
    "                                                            dec_enc_mask=dec_enc_mask,\n",
    "                                                            non_pad_mask=non_pad_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 256]), torch.Size([32, 3, 3]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_output.size(), enc_attn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 256]), torch.Size([32, 4, 4]), torch.Size([32, 4, 3]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_output.size(), dec_self_attn.size(), dec_enc_attn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 5, 3],\n",
       "        [1, 3, 0],\n",
       "        [5, 3, 6],\n",
       "        [3, 6, 2]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_mask(attn, batch=0, head=None, stop=None, n_head=8):\n",
    "    attns = attn.chunk(n_head, dim=0)\n",
    "    if head is None:\n",
    "        assert stop is not None, 'insert stop iter for n_head'\n",
    "        for i in range(0, stop):\n",
    "            print(attns[i][batch])\n",
    "            plt.imshow(attns[i][batch].detach().numpy())\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(attns[head][batch])\n",
    "        plt.imshow(attns[head][batch].detach().numpy())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 1.0000, 0.0000],\n",
      "        [0.9999, 0.0001, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 1.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_mask(enc_attn, batch=1, stop=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0],\n",
       "        [5, 7, 9, 2],\n",
       "        [3, 7, 0, 0],\n",
       "        [2, 9, 4, 0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAD8CAYAAACPd+p5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADJBJREFUeJzt3W/MnXV9x/H3Z6WUISp/E2rbgQvEzbkJ0nQYkoWARGgMXSJm8EDBQLoYmbhsyXRLWOYj3ANNDMalATIwRjFFWWcwpIQaNRtIbUql7cCOZKFdM7BgoVFxJd89OBfs5va++TWci+ucw/1+JSf39efH+f5OIB/Ouf59U1VI0mv5rUlPQNL0MygkNRkUkpoMCklNBoWkJoNCUtNYQZHk1CRbk/y0+3vKIuNeSrKze20Zp6ak4WWc6yiS/CPwbFXdkuQzwClV9TcLjDtSVSeNMU9JEzRuUDwOXFxVB5OsBL5XVe9aYJxBIc2wcYPi51V1crcc4LmX1+eNOwrsBI4Ct1TVvYu830ZgI8BbTswFv3fO8a97btPqiV0nTnoK0ite4LmfVdUZrXHHtQYkeQA4c4Fdfzd3paoqyWKpc1ZVHUjyu8CDSX5SVf85f1BVbQI2Aax97wn1o/vXtKY3cz74jvMmPQXpFQ/U5v86lnHNoKiqDyy2L8n/JFk556fH04u8x4Hu75NJvgecD/xGUEiaTuOeHt0CXNstXwv8y/wBSU5JsqJbPh24CNgzZl1JAxo3KG4BLkvyU+AD3TpJ1ia5rRvz+8D2JI8C2xgdozAopBnS/OnxWqrqEHDpAtu3Azd0y/8G/OE4dSRNlldmSmoyKCQ1GRSSmgwKSU0GhaQmg0JSk0EhqcmgkNRkUEhqMigkNRkUkpoMCklNBoWkJoNCUpNBIanJoJDUZFBIajIoJDX1EhRJLk/yeJJ9Xcew+ftXJLm72/9wkrP7qCtpGGMHRZJlwJeBK4B3A9ckefe8Ydczag50DvBF4PPj1pU0nD6+UawD9lXVk1X1a+AbwIZ5YzYAd3bLm4FLu85ikmZAH0GxCnhqzvr+btuCY6rqKHAYOK2H2pIGMFUHM5NsTLI9yfZnDr006elI6vQRFAeAuU1CV3fbFhyT5Djg7cCh+W9UVZuqam1VrT3jtGU9TE1SH/oIikeAc5O8M8nxwNWMWg3ONbf14FXAgzVOG3VJgxqrUxiMjjkkuRG4H1gG3FFVu5N8DtheVVuA24GvJtkHPMsoTCTNiLGDAqCq7gPum7ft5jnLvwI+0kctScObqoOZkqaTQSGpyaCQ1GRQSGoyKCQ1GRSSmgwKSU0GhaQmg0JSk0EhqcmgkNRkUEhqMigkNRkUkpoMCklNBoWkJoNCUpNBIanJoJDUNFTv0euSPJNkZ/e6oY+6koYx9sN15/QevYxRl7BHkmypqj3zht5dVTeOW0/S8IbqPSpphg3VexTgw0l2JdmcZM0C+20pKE2poQ5m/itwdlX9EbCV/+9s/iq2FJSm0yC9R6vqUFW92K3eBlzQQ11JAxmk92iSlXNWrwT29lBX0kCG6j36qSRXAkcZ9R69bty6koYzVO/RzwKf7aOWpOF5ZaakJoNCUpNBIanJoJDUZFBIajIoJDUZFJKaDApJTQaFpCaDQlKTQSGpyaCQ1GRQSGoyKCQ1GRSSmgwKSU0GhaQmg0JSU18tBe9I8nSSxxbZnyRf6loO7kryvj7qShpGX98o/hm4/DX2XwGc2702Al/pqa6kAfQSFFX1fUZP117MBuCuGnkIOHneI/wlTbGhjlEcU9tBWwpK02mqDmbaUlCaTkMFRbPtoKTpNVRQbAE+1p39uBA4XFUHB6otaUy9dApL8nXgYuD0JPuBvweWA1TVPzHqIrYe2Af8Avh4H3UlDaOvloLXNPYX8Mk+akka3lQdzJQ0nQwKSU0GhaQmg0JSk0EhqcmgkNRkUEhqMigkNRkUkpoMCklNBoWkJoNCUpNBIanJoJDUZFBIajIoJDUZFJKaDApJTUO1FLw4yeEkO7vXzX3UlTSMXp6Zyail4K3AXa8x5gdV9aGe6kka0FAtBSXNsL6+URyL9yd5FPhv4K+ravf8AUk2MmpizAmcyAffcd6A05O0mKGCYgdwVlUdSbIeuJdRZ/NXqapNwCaAt+XUGmhukhoGOetRVc9X1ZFu+T5geZLTh6gtaXyDBEWSM5OkW17X1T00RG1J4xuqpeBVwCeSHAV+CVzddQ+TNAOGail4K6PTp5JmkFdmSmoyKCQ1GRSSmgwKSU0GhaQmg0JSk0EhqcmgkNRkUEhqMigkNRkUkpoMCklNBoWkJoNCUpNBIanJoJDUZFBIajIoJDWNHRRJ1iTZlmRPkt1JblpgTJJ8Kcm+JLuSvG/cupKG08czM48Cf1VVO5K8Ffhxkq1VtWfOmCsY9fE4F/hj4CvdX0kzYOxvFFV1sKp2dMsvAHuBVfOGbQDuqpGHgJOTrBy3tqRh9HqMIsnZwPnAw/N2rQKemrO+n98ME5JsTLI9yfb/5cU+pyZpDL0FRZKTgHuAT1fV86/nPapqU1Wtraq1y1nR19QkjamXoEiynFFIfK2qvrXAkAPAmjnrq7ttkmZAH2c9AtwO7K2qLywybAvwse7sx4XA4ao6OG5tScPo46zHRcBHgZ8k2dlt+1vgd+CVloL3AeuBfcAvgI/3UFfSQMYOiqr6IZDGmAI+OW4tSZPhlZmSmgwKSU0GhaQmg0JSk0EhqcmgkNRkUEhqMigkNRkUkpoMCklNBoWkJoNCUpNBIanJoJDUZFBIajIoJDUZFJKaDApJTUO1FLw4yeEkO7vXzePWlTScoVoKAvygqj7UQz1JAxuqpaCkGTZUS0GA9yd5NMl3k/zBIv+8LQWlKdTHTw+g2VJwB3BWVR1Jsh64l1Fn81epqk3AJoC35dTqa26SxjNIS8Gqer6qjnTL9wHLk5zeR21Jb7xBWgomObMbR5J1Xd1D49aWNIyhWgpeBXwiyVHgl8DVXfcwSTNgqJaCtwK3jltL0mR4ZaakJoNCUpNBIanJoJDUZFBIajIoJDUZFJKaDApJTQaFpCaDQlKTQSGpyaCQ1GRQSGoyKCQ1GRSSmgwKSU0GhaQmg0JSUx8P1z0hyY+6nh27k/zDAmNWJLk7yb4kD3f9PyTNiD6+UbwIXFJV7wXOAy5PcuG8MdcDz1XVOcAXgc/3UFfSQPpoKVgv9+wAlnev+U/Y3gDc2S1vBi59+fH9kqZfXw2AlnWP6n8a2FpV81sKrgKeAqiqo8Bh4LQ+akt64/USFFX1UlWdB6wG1iV5z+t5H3uPStOp17MeVfVzYBtw+bxdB4A1AEmOA97OAp3CqmpTVa2tqrXLWdHn1CSNoY+zHmckOblb/m3gMuA/5g3bAlzbLV8FPGinMGl29NFScCVwZ5JljILnm1X1nSSfA7ZX1RZGvUm/mmQf8CxwdQ91JQ2kj5aCu4DzF9h+85zlXwEfGbeWpMnwykxJTQaFpCaDQlKTQSGpyaCQ1GRQSGoyKCQ1GRSSmgwKSU0GhaQmg0JSk0EhqcmgkNRkUEhqMigkNRkUkpoMCklNBoWkJoNCUtNQvUevS/JMkp3d64Zx60oaTh9P4X659+iRJMuBHyb5blU9NG/c3VV1Yw/1JA2sj6dwF9DqPSpphvXxjYKup8ePgXOALy/QexTgw0n+BHgC+MuqemqB99kIbOxWjzxQmx/vY37H6HTgZwPWG4qfa/YM+dnOOpZB6bNhV9cx7NvAX1TVY3O2nwYcqaoXk/w58GdVdUlvhXuQZHtVrZ30PPrm55o90/jZBuk9WlWHqurlrsO3ARf0WVfSG2uQ3qNJVs5ZvRLYO25dScMZqvfop5JcCRxl1Hv0uh7q9m3TpCfwBvFzzZ6p+2y9HqOQ9ObklZmSmgwKSU1LPiiSXJ7k8ST7knxm0vPpS5I7kjyd5LH26NmRZE2SbUn2dLcM3DTpOfXhWG6FmKQlfYyiOwD7BKMzNfuBR4BrqmrPRCfWg+7itiPAXVX1nknPpy/dGbSVVbUjyVsZXej3p7P+7yxJgLfMvRUCuGmBWyEmYql/o1gH7KuqJ6vq18A3gA0TnlMvqur7jM4wvalU1cGq2tEtv8DoVPuqyc5qfDUytbdCLPWgWAXMvZR8P2+C/+iWiiRnA+cDC90yMHOSLEuyE3ga2LrIrRATsdSDQjMqyUnAPcCnq+r5Sc+nD1X1UlWdB6wG1iWZmp+MSz0oDgBr5qyv7rZpinW/4e8BvlZV35r0fPq22K0Qk7TUg+IR4Nwk70xyPHA1sGXCc9Jr6A763Q7sraovTHo+fTmWWyEmaUkHRVUdBW4E7md0UOybVbV7srPqR5KvA/8OvCvJ/iTXT3pOPbkI+ChwyZwnpq2f9KR6sBLYlmQXo/+Bba2q70x4Tq9Y0qdHJR2bJf2NQtKxMSgkNRkUkpoMCklNBoWkJoNCUpNBIanp/wCmhPyCuiHpwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAD8CAYAAACPd+p5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADJBJREFUeJzt3W/MnXV9x/H3Z6WUISp/E2rbgQvEzbkJ0nQYkoWARGgMXSJm8EDBQLoYmbhsyXRLWOYj3ANNDMalATIwRjFFWWcwpIQaNRtIbUql7cCOZKFdM7BgoVFxJd89OBfs5va++TWci+ucw/1+JSf39efH+f5OIB/Ouf59U1VI0mv5rUlPQNL0MygkNRkUkpoMCklNBoWkJoNCUtNYQZHk1CRbk/y0+3vKIuNeSrKze20Zp6ak4WWc6yiS/CPwbFXdkuQzwClV9TcLjDtSVSeNMU9JEzRuUDwOXFxVB5OsBL5XVe9aYJxBIc2wcYPi51V1crcc4LmX1+eNOwrsBI4Ct1TVvYu830ZgI8BbTswFv3fO8a97btPqiV0nTnoK0ite4LmfVdUZrXHHtQYkeQA4c4Fdfzd3paoqyWKpc1ZVHUjyu8CDSX5SVf85f1BVbQI2Aax97wn1o/vXtKY3cz74jvMmPQXpFQ/U5v86lnHNoKiqDyy2L8n/JFk556fH04u8x4Hu75NJvgecD/xGUEiaTuOeHt0CXNstXwv8y/wBSU5JsqJbPh24CNgzZl1JAxo3KG4BLkvyU+AD3TpJ1ia5rRvz+8D2JI8C2xgdozAopBnS/OnxWqrqEHDpAtu3Azd0y/8G/OE4dSRNlldmSmoyKCQ1GRSSmgwKSU0GhaQmg0JSk0EhqcmgkNRkUEhqMigkNRkUkpoMCklNBoWkJoNCUpNBIanJoJDUZFBIajIoJDX1EhRJLk/yeJJ9Xcew+ftXJLm72/9wkrP7qCtpGGMHRZJlwJeBK4B3A9ckefe8Ydczag50DvBF4PPj1pU0nD6+UawD9lXVk1X1a+AbwIZ5YzYAd3bLm4FLu85ikmZAH0GxCnhqzvr+btuCY6rqKHAYOK2H2pIGMFUHM5NsTLI9yfZnDr006elI6vQRFAeAuU1CV3fbFhyT5Djg7cCh+W9UVZuqam1VrT3jtGU9TE1SH/oIikeAc5O8M8nxwNWMWg3ONbf14FXAgzVOG3VJgxqrUxiMjjkkuRG4H1gG3FFVu5N8DtheVVuA24GvJtkHPMsoTCTNiLGDAqCq7gPum7ft5jnLvwI+0kctScObqoOZkqaTQSGpyaCQ1GRQSGoyKCQ1GRSSmgwKSU0GhaQmg0JSk0EhqcmgkNRkUEhqMigkNRkUkpoMCklNBoWkJoNCUpNBIanJoJDUNFTv0euSPJNkZ/e6oY+6koYx9sN15/QevYxRl7BHkmypqj3zht5dVTeOW0/S8IbqPSpphg3VexTgw0l2JdmcZM0C+20pKE2poQ5m/itwdlX9EbCV/+9s/iq2FJSm0yC9R6vqUFW92K3eBlzQQ11JAxmk92iSlXNWrwT29lBX0kCG6j36qSRXAkcZ9R69bty6koYzVO/RzwKf7aOWpOF5ZaakJoNCUpNBIanJoJDUZFBIajIoJDUZFJKaDApJTQaFpCaDQlKTQSGpyaCQ1GRQSGoyKCQ1GRSSmgwKSU0GhaQmg0JSU18tBe9I8nSSxxbZnyRf6loO7kryvj7qShpGX98o/hm4/DX2XwGc2702Al/pqa6kAfQSFFX1fUZP117MBuCuGnkIOHneI/wlTbGhjlEcU9tBWwpK02mqDmbaUlCaTkMFRbPtoKTpNVRQbAE+1p39uBA4XFUHB6otaUy9dApL8nXgYuD0JPuBvweWA1TVPzHqIrYe2Af8Avh4H3UlDaOvloLXNPYX8Mk+akka3lQdzJQ0nQwKSU0GhaQmg0JSk0EhqcmgkNRkUEhqMigkNRkUkpoMCklNBoWkJoNCUpNBIanJoJDUZFBIajIoJDUZFJKaDApJTUO1FLw4yeEkO7vXzX3UlTSMXp6Zyail4K3AXa8x5gdV9aGe6kka0FAtBSXNsL6+URyL9yd5FPhv4K+ravf8AUk2MmpizAmcyAffcd6A05O0mKGCYgdwVlUdSbIeuJdRZ/NXqapNwCaAt+XUGmhukhoGOetRVc9X1ZFu+T5geZLTh6gtaXyDBEWSM5OkW17X1T00RG1J4xuqpeBVwCeSHAV+CVzddQ+TNAOGail4K6PTp5JmkFdmSmoyKCQ1GRSSmgwKSU0GhaQmg0JSk0EhqcmgkNRkUEhqMigkNRkUkpoMCklNBoWkJoNCUpNBIanJoJDUZFBIajIoJDWNHRRJ1iTZlmRPkt1JblpgTJJ8Kcm+JLuSvG/cupKG08czM48Cf1VVO5K8Ffhxkq1VtWfOmCsY9fE4F/hj4CvdX0kzYOxvFFV1sKp2dMsvAHuBVfOGbQDuqpGHgJOTrBy3tqRh9HqMIsnZwPnAw/N2rQKemrO+n98ME5JsTLI9yfb/5cU+pyZpDL0FRZKTgHuAT1fV86/nPapqU1Wtraq1y1nR19QkjamXoEiynFFIfK2qvrXAkAPAmjnrq7ttkmZAH2c9AtwO7K2qLywybAvwse7sx4XA4ao6OG5tScPo46zHRcBHgZ8k2dlt+1vgd+CVloL3AeuBfcAvgI/3UFfSQMYOiqr6IZDGmAI+OW4tSZPhlZmSmgwKSU0GhaQmg0JSk0EhqcmgkNRkUEhqMigkNRkUkpoMCklNBoWkJoNCUpNBIanJoJDUZFBIajIoJDUZFJKaDApJTUO1FLw4yeEkO7vXzePWlTScoVoKAvygqj7UQz1JAxuqpaCkGTZUS0GA9yd5NMl3k/zBIv+8LQWlKdTHTw+g2VJwB3BWVR1Jsh64l1Fn81epqk3AJoC35dTqa26SxjNIS8Gqer6qjnTL9wHLk5zeR21Jb7xBWgomObMbR5J1Xd1D49aWNIyhWgpeBXwiyVHgl8DVXfcwSTNgqJaCtwK3jltL0mR4ZaakJoNCUpNBIanJoJDUZFBIajIoJDUZFJKaDApJTQaFpCaDQlKTQSGpyaCQ1GRQSGoyKCQ1GRSSmgwKSU0GhaQmg0JSUx8P1z0hyY+6nh27k/zDAmNWJLk7yb4kD3f9PyTNiD6+UbwIXFJV7wXOAy5PcuG8MdcDz1XVOcAXgc/3UFfSQPpoKVgv9+wAlnev+U/Y3gDc2S1vBi59+fH9kqZfXw2AlnWP6n8a2FpV81sKrgKeAqiqo8Bh4LQ+akt64/USFFX1UlWdB6wG1iV5z+t5H3uPStOp17MeVfVzYBtw+bxdB4A1AEmOA97OAp3CqmpTVa2tqrXLWdHn1CSNoY+zHmckOblb/m3gMuA/5g3bAlzbLV8FPGinMGl29NFScCVwZ5JljILnm1X1nSSfA7ZX1RZGvUm/mmQf8CxwdQ91JQ2kj5aCu4DzF9h+85zlXwEfGbeWpMnwykxJTQaFpCaDQlKTQSGpyaCQ1GRQSGoyKCQ1GRSSmgwKSU0GhaQmg0JSk0EhqcmgkNRkUEhqMigkNRkUkpoMCklNBoWkJoNCUtNQvUevS/JMkp3d64Zx60oaTh9P4X659+iRJMuBHyb5blU9NG/c3VV1Yw/1JA2sj6dwF9DqPSpphvXxjYKup8ePgXOALy/QexTgw0n+BHgC+MuqemqB99kIbOxWjzxQmx/vY37H6HTgZwPWG4qfa/YM+dnOOpZB6bNhV9cx7NvAX1TVY3O2nwYcqaoXk/w58GdVdUlvhXuQZHtVrZ30PPrm55o90/jZBuk9WlWHqurlrsO3ARf0WVfSG2uQ3qNJVs5ZvRLYO25dScMZqvfop5JcCRxl1Hv0uh7q9m3TpCfwBvFzzZ6p+2y9HqOQ9ObklZmSmgwKSU1LPiiSXJ7k8ST7knxm0vPpS5I7kjyd5LH26NmRZE2SbUn2dLcM3DTpOfXhWG6FmKQlfYyiOwD7BKMzNfuBR4BrqmrPRCfWg+7itiPAXVX1nknPpy/dGbSVVbUjyVsZXej3p7P+7yxJgLfMvRUCuGmBWyEmYql/o1gH7KuqJ6vq18A3gA0TnlMvqur7jM4wvalU1cGq2tEtv8DoVPuqyc5qfDUytbdCLPWgWAXMvZR8P2+C/+iWiiRnA+cDC90yMHOSLEuyE3ga2LrIrRATsdSDQjMqyUnAPcCnq+r5Sc+nD1X1UlWdB6wG1iWZmp+MSz0oDgBr5qyv7rZpinW/4e8BvlZV35r0fPq22K0Qk7TUg+IR4Nwk70xyPHA1sGXCc9Jr6A763Q7sraovTHo+fTmWWyEmaUkHRVUdBW4E7md0UOybVbV7srPqR5KvA/8OvCvJ/iTXT3pOPbkI+ChwyZwnpq2f9KR6sBLYlmQXo/+Bba2q70x4Tq9Y0qdHJR2bJf2NQtKxMSgkNRkUkpoMCklNBoWkJoNCUpNBIanp/wCmhPyCuiHpwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_mask(dec_self_attn, batch=2, stop=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3781, 0.3450, 0.2769],\n",
      "        [0.3340, 0.4613, 0.2047],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6594, 0.2007, 0.1399],\n",
      "        [0.3593, 0.4861, 0.1546],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1682, 0.5437, 0.2881],\n",
      "        [0.1661, 0.3735, 0.4604],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0825, 0.4967, 0.4208],\n",
      "        [0.5540, 0.2102, 0.2357],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_mask(dec_enc_attn, batch=2, stop=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models: Encoder & Decoder - Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_len, max_seq_len, n_layer, n_head, d_model, d_k, d_v, d_f, \n",
    "                 drop_rate=0.1, use_conv=False, return_attn=True, pad_idx=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.return_attn = return_attn\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.embed_layer = Embedding(vocab_len, d_model, pad_idx=pad_idx)\n",
    "        self.pos_layer = PositionalEncoding(max_seq_len+1, d_model, pad_idx=0)\n",
    "        self.layers = nn.ModuleList([Encode_Layer(n_head, d_model, d_k, d_v, d_f, \n",
    "                                                  drop_rate=drop_rate, \n",
    "                                                  use_conv=use_conv,\n",
    "                                                  return_attn=return_attn) \\\n",
    "                                     for i in range(n_layer)])\n",
    "        \n",
    "    def forward(self, enc, enc_pos):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        * enc: (B, T)\n",
    "        * enc_pos: (B, T)\n",
    "        -------------------------------------\n",
    "        Outputs:\n",
    "        * enc_output: (B, T, d_model)\n",
    "        * self_attns: (n_head*B, T, T)\n",
    "        \"\"\"\n",
    "        self_attns = []  # (n_layer, n_head*B, T, T)\n",
    "        # self attention padding mask: (B, T, T)\n",
    "        attn_mask = get_padding_mask(q=enc, k=enc, pad_idx=self.pad_idx, mode='attn')\n",
    "        non_pad_mask = get_padding_mask(q=enc, pad_idx=self.pad_idx, mode='nonpad')\n",
    "        # embedding + position encoding: (B, T) --> (B, T, d_model)\n",
    "        enc_output = self.embed_layer(enc) + self.pos_layer(enc_pos)\n",
    "        enc_output = self.dropout(enc_output)\n",
    "        # forward encode layer\n",
    "        for enc_layer in self.layers:\n",
    "            if self.return_attn:\n",
    "                enc_output, enc_self_attn = enc_layer(enc_input=enc_output, \n",
    "                                                      enc_mask=attn_mask, \n",
    "                                                      non_pad_mask=non_pad_mask)\n",
    "                self_attns.append(enc_self_attn)\n",
    "            else:\n",
    "                enc_output = enc_layer(enc_input=enc_output, \n",
    "                                       enc_mask=attn_mask, \n",
    "                                       non_pad_mask=non_pad_mask)\n",
    "        \n",
    "        if self.return_attn:\n",
    "            return enc_output, self_attns\n",
    "        return enc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_len, max_seq_len, n_layer, n_head, d_model, d_k, d_v, d_f, \n",
    "                 pad_idx=0, drop_rate=0.1, use_conv=False, return_attn=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.return_attn = return_attn\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.embed_layer = Embedding(vocab_len, d_model, pad_idx=pad_idx)\n",
    "        self.pos_layer = PositionalEncoding(max_seq_len+1, d_model, pad_idx=0)\n",
    "        self.layers = nn.ModuleList([Decode_Layer(n_head, d_model, d_k, d_v, d_f, \n",
    "                                                  drop_rate=drop_rate, \n",
    "                                                  use_conv=use_conv,\n",
    "                                                  return_attn=return_attn) \\\n",
    "                                     for i in range(n_layer)])\n",
    "        \n",
    "    def forward(self, dec, dec_pos, enc, enc_output):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        * dec: (B, T_q)\n",
    "        * dec_pos: (B, T_q)\n",
    "        * enc: (B, T)\n",
    "        * enc_output: (B, T, d_model)\n",
    "        -------------------------------------\n",
    "        Outputs:\n",
    "        * dec_output: (B, T_q, d_model)\n",
    "        * self_attns: (n_head*B, T_q, T_q)\n",
    "        * dec_enc_attns: (n_haed*B, T_q, T)\n",
    "        \"\"\"\n",
    "        self_attns = []  # (n_layer, n_head*B, T_q, T_q)\n",
    "        dec_enc_attns = []  # (n_layer, n_head*B, T_q, T)\n",
    "        \n",
    "        # self attention padding mask: (B, T_q, T)\n",
    "        non_pad_mask = get_padding_mask(q=dec, pad_idx=self.pad_idx, mode='nonpad')\n",
    "        attn_mask = get_padding_mask(q=dec, k=dec, pad_idx=self.pad_idx, mode='attn')\n",
    "        subseq_mask = get_padding_mask(q=dec, mode='subseq')\n",
    "        self_attn_mask = (attn_mask + subseq_mask).gt(0)\n",
    "        # enc_dec attention padding mask\n",
    "        dec_enc_attn_mask = get_padding_mask(q=dec, k=enc, pad_idx=self.pad_idx, mode='attn')\n",
    "        \n",
    "        # embedding + position encoding: (B, T) --> (B, T, d_model)\n",
    "        dec_output = self.embed_layer(dec) + self.pos_layer(dec_pos)\n",
    "        dec_output = self.dropout(dec_output)\n",
    "        # forward decode layer\n",
    "        for dec_layer in self.layers:\n",
    "            if self.return_attn:\n",
    "                dec_output, dec_self_attn, dec_enc_attn = dec_layer(dec_input=dec_output, \n",
    "                                                                    enc_output=enc_output, \n",
    "                                                                    dec_self_mask=self_attn_mask, \n",
    "                                                                    dec_enc_mask=dec_enc_attn_mask,\n",
    "                                                                    non_pad_mask=non_pad_mask)\n",
    "                self_attns.append(dec_self_attn)\n",
    "                dec_enc_attns.append(dec_enc_attn)\n",
    "            else:\n",
    "                dec_output = dec_layer(dec_input=dec_output, \n",
    "                                       enc_output=enc_output, \n",
    "                                       dec_self_mask=self_attn_mask, \n",
    "                                       dec_enc_mask=dec_enc_attn_mask,\n",
    "                                       non_pad_mask=non_pad_mask)\n",
    "        \n",
    "        if self.return_attn:\n",
    "            return dec_output, self_attns, dec_enc_attns\n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = 3\n",
    "encoder = Encoder(vocab_len, 3, n_layer, n_head, d_model, d_k, d_v, d_f)\n",
    "decoder = Decoder(vocab_len, 4, n_layer, n_head, d_model, d_k, d_v, d_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_output, enc_self_attns = encoder(x, po_x)\n",
    "dec_output, dec_self_attns, dec_enc_attns = decoder.forward(t, po_t, x, enc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 256]), torch.Size([4, 4, 256]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_output.size(), dec_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, enc_vocab_len, enc_max_seq_len, dec_vocab_len, dec_max_seq_len, \n",
    "                 n_layer, n_head, d_model, d_k, d_v, d_f, \n",
    "                 pad_idx=0, drop_rate=0.1, use_conv=False, return_attn=True,\n",
    "                 linear_weight_share=True, embed_weight_share=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.return_attn = return_attn\n",
    "        self.pad_idx = pad_idx\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.encoder = Encoder(enc_vocab_len, enc_max_seq_len, n_layer, n_head, \n",
    "                               d_model, d_k, d_v, d_f, \n",
    "                               pad_idx=pad_idx, \n",
    "                               drop_rate=drop_rate, \n",
    "                               use_conv=use_conv, \n",
    "                               return_attn=return_attn)\n",
    "        self.decoder = Decoder(dec_vocab_len, dec_max_seq_len, n_layer, n_head, \n",
    "                               d_model, d_k, d_v, d_f,\n",
    "                               pad_idx=pad_idx, \n",
    "                               drop_rate=drop_rate, \n",
    "                               use_conv=use_conv, \n",
    "                               return_attn=return_attn)\n",
    "        self.projection = XavierLinear(d_model, dec_vocab_len, bias=False)\n",
    "        if linear_weight_share:\n",
    "            # share the same weight matrix between the decoder embedding layer \n",
    "            # and the pre-softmax linear transformation\n",
    "            self.projection.linear.weight = self.decoder.embed_layer.embedding.weight\n",
    "        \n",
    "        if embed_weight_share:\n",
    "            # share the same weight matrix between the decoder embedding layer \n",
    "            # and the encoder embedding layer\n",
    "            assert enc_vocab_len == dec_vocab_len, \"vocab length must be same\"\n",
    "            self.encoder.embed_layer.embedding.weight = self.decoder.embed_layer.embedding.weight\n",
    "            \n",
    "    def forward(self, enc, enc_pos, dec, dec_pos):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        * enc: (B, T)\n",
    "        * enc_pos: (B, T)\n",
    "        * dec: (B, T_q)\n",
    "        * dec_pos: (B, T_q)\n",
    "        -------------------------------------\n",
    "        Outputs:\n",
    "        * dec_output: (B*T_q, d_model)\n",
    "        * attns_dict:\n",
    "            * enc_self_attns: (n_head*B, T, T)\n",
    "            * dec_self_attns: (n_head*B, T_q, T_q)\n",
    "            * dec_enc_attns: (n_haed*B, T_q, T)\n",
    "        \"\"\"\n",
    "        if self.return_attn:\n",
    "            enc_output, enc_self_attns = self.encoder(enc, enc_pos)\n",
    "            dec_output, dec_self_attns, dec_enc_attns = self.decoder(dec, dec_pos, enc, enc_output)\n",
    "            dec_output = self.projection(dec_output)\n",
    "            attns_dict = {'enc_self_attns': enc_self_attns, \n",
    "                         'dec_self_attns': dec_self_attns,\n",
    "                         'dec_enc_attns': dec_enc_attns}\n",
    "            return dec_output.view(-1, dec_output.size(2)), attns_dict\n",
    "        else:\n",
    "            enc_output = self.encoder(enc, enc_pos)\n",
    "            dec_output = self.decoder(dec, dec_pos, enc, enc_output)\n",
    "            dec_output = self.projection(dec_output)\n",
    "            return dec_output.view(-1, dec_output.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(vocab_len, 3, vocab_len, 4, n_layer, n_head, d_model, d_k, d_v, d_f, \n",
    "                 pad_idx=0, drop_rate=0.1, use_conv=False, return_attn=False,\n",
    "                 linear_weight_share=True, embed_weight_share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(x, po_x, t, po_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ref: https://arxiv.org/pdf/1512.00567.pdf\n",
    "\n",
    "cross-entropy: $l = - \\sum_{k=1}^{K} \\log(p(k))q(k)$\n",
    "\n",
    "$\\dfrac{\\partial l}{\\partial z_{k}} = p(k) - q(k)$ where $q(k) = \\delta_{k, y}$\n",
    "\n",
    "$q(k)$ is not achivable, where $z_y \\gg z_k$ where $k \\neq y$\n",
    "\n",
    "Only using cross-entropy loss cause two problems. First, it may result in over-fitting: if the model learns to assign full probability to the groundtruth label for each training example, it is not guaranteed to generalize. Second, it encourages the differences between the largest logit and all others to become large, and this, combined with the bounded gradient $\\frac{\\partial l}{\\partial z_{k}}$, reduces the ability of the model to adapt. Intuitively, this happens because the model becomes too confident about its predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### proposed method: label-smoothing regularization\n",
    "\n",
    "* $u(k)$: a distribution over labels, independent of the training example $x$\n",
    "* $\\epsilon$: a smoothing parameter\n",
    "\n",
    "For a training example with ground-truth label $y$, replace the label distribution $q(k) = \\delta_{k, y}$ to\n",
    "\n",
    "$$q'(k \\vert x) = (1-\\epsilon)\\delta_{k, y}+\\epsilon u(k)$$\n",
    "\n",
    "This can be seen as the distribution of the label $k$ obtained as follows: first, set it to the groundtruth label $k = y$; then, with probability $\\epsilon$, replace $k$ with a sample drawn from the distribution $u(k)$. We propose to use the prior distribution over labels as $u(k)$. In our experiments, we used the uniform distribution $u(k) = 1/K$, so that\n",
    "\n",
    "$$q'(k \\vert x) = (1-\\epsilon)\\delta_{k, y}+ \\dfrac{\\epsilon}{K}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interpretation: $q'$ is changed true probability.\n",
    "\n",
    "$$\\begin{aligned} H(q', p) &= -\\sum_{k=1}^K q'(k) \\log p(k)  \\\\\n",
    "&= -\\sum_{k=1}^K \\big( ((1-\\epsilon)q(x) + \\epsilon u(x)) \\log p(k) \\big) \\\\\n",
    "&= -\\sum_{k=1}^K (1-\\epsilon)q(x) \\log p(k) + -\\sum_{k=1}^K \\epsilon u(x) \\log p(k) \\\\\n",
    "&= (1-\\epsilon) H(q, p) + \\epsilon H(u, p) \\end{aligned}$$\n",
    "\n",
    "this can be seen as $H(q', p) = H(q, p) + \\dfrac{\\epsilon}{(1-\\epsilon) } \\big( D_{KL}(u \\vert\\vert p) + H(u) \\big) $ and $H(u)$ is fixed. When $u$ is the uniform distribution, $H(u, p)$ is a measure of how dissimilar the predicted distribution $p$ is to uniform, which could also be measured (but not equivalently) by negative entropy $H(p)$;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"Label Smoothing\"\"\"\n",
    "    def __init__(self, trg_vocab_size, pad_idx, eps=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "#         self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "        self.pad_idx = pad_idx\n",
    "        self.eps = eps\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        x: (B, T_q, V_target), scores\n",
    "        t: (B, T_q)\n",
    "        ref: https://arxiv.org/pdf/1512.00567.pdf  \n",
    "        #7\n",
    "        q'(k | x) = (1 - eps) q(k) + eps * u(k)\n",
    "        where: q(k) = 1 if k == 'target class' else 0\n",
    "        \"\"\"\n",
    "        \n",
    "        assert x.size(1) == self.trg_vocab_size, \\\n",
    "            'vocab size is not equal x: {}, vocab: {}'.format(x.size(1), self.trg_vocab_size)\n",
    "        assert target.dim() == 2, 't must be size of (B, T_q)'\n",
    "        if self.eps == 0.0:\n",
    "            x = x.view(-1, x.size(2))\n",
    "            return self.criterion(x, target.view(-1))\n",
    "        \n",
    "        true_dist = torch.zeros_like(x)\n",
    "        true_dist = true_dist.scatter(1, target.view(-1, 1), 1.0)\n",
    "        true_dist = (1 - self.eps) * true_dist + self.eps / (self.trg_vocab_size -1)\n",
    "        log_prob = F.log_softmax(x, dim=1)\n",
    "        loss = -(true_dist * log_prob).sum(1)\n",
    "        non_pad_mask = target.view(-1).ne(self.pad_idx)\n",
    "        self.true_dist = true_dist\n",
    "        loss = loss.masked_select(non_pad_mask).sum()\n",
    "        return loss\n",
    "        \n",
    "#         true_dist = x.clone()\n",
    "#         # u(k) = 1 / K, exclude token <s>, <pad>\n",
    "#         true_dist.fill_(self.eps / (self.trg_vocab_size - 2))  \n",
    "#         # at target index, value is (1-eps) * 1\n",
    "#         true_dist.scatter_(1, target.view(-1, 1), self.confidence)\n",
    "#         true_dist[:, self.pad_idx] = 0  # exclude token <s>\n",
    "#         mask = torch.nonzero(target.view(-1) == self.pad_idx)\n",
    "#         if mask.dim() > 0:\n",
    "#             true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "#         self.true_dist = true_dist\n",
    "#         return self.criterion(x.log_softmax(1), true_dist.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbs = LabelSmoothing(10, 0, eps=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(65.3785, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbs(output, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "# one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "# log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "# non_pad_mask = gold.ne(Constants.PAD)\n",
    "# loss = -(one_hot * log_prb).sum(dim=1)\n",
    "# loss = loss.masked_select(non_pad_mask).sum()  # average later"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
