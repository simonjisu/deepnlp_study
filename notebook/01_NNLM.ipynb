{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Probabilistic Language Model\n",
    "---\n",
    "Paper implementation: \n",
    "\n",
    "* paper: [A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) - Yoshua Bengio, 2003\n",
    "* blog: []()\n",
    "* [slide share](http://bit.ly/2OkYFkY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Preprocessing\n",
    "2. Model\n",
    "3. Result: \n",
    "    * Perplexity\n",
    "    * Similarity versus \"gensim Word2Vec\"\n",
    "    * Training Time\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/'.join(os.getcwd().split('/')[:-1]+['paper_code', 'NNLM']))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from nnlm_data_loader import DataSet\n",
    "from model import NNLM\n",
    "from konlpy.tag import Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = \"cuda\"\n",
    "# USE_CUDA = False\n",
    "# DEVICE = None\n",
    "BATCH = 1024\n",
    "N_GRAM = 5\n",
    "TAGGER = lambda x: ['/'.join(y) for y in Twitter().pos(x, norm=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, valid_data, test_data = DataSet(base_path='../data/nsmc/', train='train.txt', valid='valid.txt',\n",
    "#                                 test='test.txt', n_gram=5, tokenizer=TAGGER, \n",
    "#                                 save_tokens=True, direct_load=False).splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_creator = DataSet(base_path='../data/nsmc/', train='train_tokens', valid='valid_tokens', test='test_tokens',\n",
    "                          n_gram=N_GRAM, tokenizer=str.split, save_tokens=False, \n",
    "                          direct_load=True, remove_short=True, device=DEVICE)\n",
    "train_data, valid_data, test_data = dataset_creator.splits()\n",
    "train_loader, valid_loader, test_loader = dataset_creator.create_loader(train=train_data, valid=valid_data, test=test_data,\n",
    "                                                                        batch_size=BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2167155, 124660)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking removed short sentences which length of tokens is lower then N_GRAM(=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180000, 17727)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual used, number of train sentences: 162273\n",
    "train_data._total, train_data._removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 968)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual used, number of valid sentences: 9032\n",
    "valid_data._total, valid_data._removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is 63202\n"
     ]
    }
   ],
   "source": [
    "# 16383 words : 30 ~~ 63202: 100\n",
    "V = len(train_data.vocab)\n",
    "E = 100\n",
    "H = 500\n",
    "LR = 0.001\n",
    "WD = 0.00001\n",
    "STEP = 10\n",
    "print(\"vocab size is\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnlm = NNLM(embed_size=E, hidden_size=H, vocab_size=V, num_prev_tokens=(N_GRAM-1))\n",
    "if USE_CUDA:\n",
    "    nnlm = nnlm.cuda()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(nnlm.parameters(), lr=LR, weight_decay=WD)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(gamma=0.1, milestones=[3, 7], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(x):\n",
    "    return -torch.log(x).sum() / x.size(0)\n",
    "\n",
    "def validation(model, loader):\n",
    "    model.eval()\n",
    "    pp = 0\n",
    "    acc = 0\n",
    "    for batch in loader:\n",
    "        inputs, targets = batch[0], batch[1]\n",
    "        preds = model.predict(inputs)\n",
    "        probs, idxes = preds.max(1)\n",
    "        acc += torch.eq(idxes, targets).sum().item()\n",
    "        pp += perplexity(probs).item()\n",
    "        \n",
    "    return acc, pp/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][0/2117] train_loss: 11.0902\n",
      "[1/10][1000/2117] train_loss: 6.5682\n",
      "[1/10][2000/2117] train_loss: 6.2932\n",
      "==============================\n",
      "[1/10]\n",
      " valid_perplextiy: 2.1775 \n",
      " valid_accuracy: 0.1725\n",
      "==============================\n",
      "[2/10][0/2117] train_loss: 5.8145\n",
      "[2/10][1000/2117] train_loss: 5.3585\n",
      "[2/10][2000/2117] train_loss: 5.3894\n",
      "==============================\n",
      "[2/10]\n",
      " valid_perplextiy: 2.0385 \n",
      " valid_accuracy: 0.1912\n",
      "==============================\n",
      "[3/10][0/2117] train_loss: 5.3993\n",
      "[3/10][1000/2117] train_loss: 5.0243\n",
      "[3/10][2000/2117] train_loss: 5.0890\n",
      "==============================\n",
      "[3/10]\n",
      " valid_perplextiy: 1.9726 \n",
      " valid_accuracy: 0.1975\n",
      "==============================\n",
      "[4/10][0/2117] train_loss: 5.1797\n",
      "[4/10][1000/2117] train_loss: 4.8227\n",
      "[4/10][2000/2117] train_loss: 4.7120\n",
      "==============================\n",
      "[4/10]\n",
      " valid_perplextiy: 2.0182 \n",
      " valid_accuracy: 0.2059\n",
      "==============================\n",
      "[5/10][0/2117] train_loss: 4.9477\n",
      "[5/10][1000/2117] train_loss: 4.6866\n",
      "[5/10][2000/2117] train_loss: 4.6311\n",
      "==============================\n",
      "[5/10]\n",
      " valid_perplextiy: 2.0121 \n",
      " valid_accuracy: 0.2068\n",
      "==============================\n",
      "[6/10][0/2117] train_loss: 4.8669\n",
      "[6/10][1000/2117] train_loss: 4.6215\n",
      "[6/10][2000/2117] train_loss: 4.5899\n",
      "==============================\n",
      "[6/10]\n",
      " valid_perplextiy: 2.0058 \n",
      " valid_accuracy: 0.2069\n",
      "==============================\n",
      "[7/10][0/2117] train_loss: 4.8080\n",
      "[7/10][1000/2117] train_loss: 4.5734\n",
      "[7/10][2000/2117] train_loss: 4.5578\n",
      "==============================\n",
      "[7/10]\n",
      " valid_perplextiy: 1.9996 \n",
      " valid_accuracy: 0.2070\n",
      "==============================\n",
      "[8/10][0/2117] train_loss: 4.7592\n",
      "[8/10][1000/2117] train_loss: 4.6040\n",
      "[8/10][2000/2117] train_loss: 4.5328\n",
      "==============================\n",
      "[8/10]\n",
      " valid_perplextiy: 2.0062 \n",
      " valid_accuracy: 0.2068\n",
      "==============================\n",
      "[9/10][0/2117] train_loss: 4.7009\n",
      "[9/10][1000/2117] train_loss: 4.5582\n",
      "[9/10][2000/2117] train_loss: 4.5126\n",
      "==============================\n",
      "[9/10]\n",
      " valid_perplextiy: 2.0068 \n",
      " valid_accuracy: 0.2062\n",
      "==============================\n",
      "[10/10][0/2117] train_loss: 4.6785\n",
      "[10/10][1000/2117] train_loss: 4.5393\n",
      "[10/10][2000/2117] train_loss: 4.5046\n",
      "==============================\n",
      "[10/10]\n",
      " valid_perplextiy: 2.0063 \n",
      " valid_accuracy: 0.2059\n",
      "==============================\n",
      "Training Excution time with validation: 34 m 51.3324 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for step in range(STEP):\n",
    "    nnlm.train()\n",
    "    scheduler.step()\n",
    "    losses=[]\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, targets = batch[0], batch[1]\n",
    "\n",
    "        nnlm.zero_grad()\n",
    "\n",
    "        outputs = nnlm(inputs)\n",
    "\n",
    "        loss = loss_function(outputs, targets.view(-1))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(nnlm.parameters(), 50.0)  # gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            msg = '[{}/{}][{}/{}] train_loss: {:.4f}'.format(step+1, STEP, i, len(train_loader), np.mean(losses))\n",
    "            print(msg)\n",
    "            \n",
    "    acc_valid, pp_valid = validation(model=nnlm, loader=valid_loader)\n",
    "    print('='*30)\n",
    "    msg = '[{}/{}]\\n valid_perplextiy: {:.4f} \\n valid_accuracy: {:.4f}'.format(step+1, STEP, pp_valid, acc_valid/len(valid_data))\n",
    "    print(msg)\n",
    "    print('='*30)\n",
    "    \n",
    "end_time = time.time()\n",
    "minute = int((end_time-start_time) // 60)\n",
    "print('Training Excution time with validation: {:d} m {:.4f} s'.format(minute, (end_time-start_time)-minute*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nnlm.state_dict(), '../paper_code/NNLM/model/nnlm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnlm.load_state_dict(torch.load('../paper_code/NNLM/model/nnlm.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_perplextiy: 2.0049, test_accuracy: 0.2076\n"
     ]
    }
   ],
   "source": [
    "acc, pp = validation(model=nnlm, loader=test_loader)\n",
    "msg = 'test_perplextiy: {:.4f}, test_accuracy: {:.4f}'.format(pp, acc/len(test_data))\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = '요즘 나오는 어린이 영화보다 수준 낮은 시나리오 거기다 우리가 아는 윌스미스 보다 어린 윌스미스에 발연기는 보너스'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_sample(test_sent, model, dataset):\n",
    "    test_sent_tokens = TAGGER(dataset_creator._preprocessing(test_sent))\n",
    "    test_sent_ngrams = dataset.get_ngrams([test_sent_tokens])\n",
    "    \n",
    "    test_sent_numerical = list(map(dataset.vocab.stoi.get, test_sent_tokens))\n",
    "    datas = np.array(dataset.get_ngrams([test_sent_numerical]))\n",
    "    \n",
    "    x = torch.LongTensor(datas[:, :-1])\n",
    "    y = torch.LongTensor(datas[:, -1])\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        y, x = y.cuda(), x.cuda()\n",
    "    \n",
    "    for tkns, inputs in zip(test_sent_ngrams, x):\n",
    "        pred = model.predict(inputs.view(1, -1)).max(1)[1]\n",
    "        print(' '.join(tkns[:-1]), '-->', dataset.vocab.itos[pred.item()], '\\t| target:', tkns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요즘/Noun 나오는/Verb 어린이/Noun 영화/Noun --> 보다/Josa \t| target: 보다/Josa\n",
      "나오는/Verb 어린이/Noun 영화/Noun 보다/Josa --> 더/Noun \t| target: 수준/Noun\n",
      "어린이/Noun 영화/Noun 보다/Josa 수준/Noun --> 이/Josa \t| target: 낮은/Adjective\n",
      "영화/Noun 보다/Josa 수준/Noun 낮은/Adjective --> 영화/Noun \t| target: 시나리오/Noun\n",
      "보다/Josa 수준/Noun 낮은/Adjective 시나리오/Noun --> 가/Josa \t| target: 거기/Noun\n",
      "수준/Noun 낮은/Adjective 시나리오/Noun 거기/Noun --> 서/Josa \t| target: 다/Josa\n",
      "낮은/Adjective 시나리오/Noun 거기/Noun 다/Josa --> ./Punctuation \t| target: 우리/Noun\n",
      "시나리오/Noun 거기/Noun 다/Josa 우리/Noun --> 의/Josa \t| target: 가/Josa\n",
      "거기/Noun 다/Josa 우리/Noun 가/Josa --> 더/Noun \t| target: 아는/Verb\n",
      "다/Josa 우리/Noun 가/Josa 아는/Verb --> 사람/Noun \t| target: 윌스미스/Noun\n",
      "우리/Noun 가/Josa 아는/Verb 윌스미스/Noun --> 를/Josa \t| target: 보다/Verb\n",
      "가/Josa 아는/Verb 윌스미스/Noun 보다/Verb --> 가/Eomi \t| target: 어린/Verb\n",
      "아는/Verb 윌스미스/Noun 보다/Verb 어린/Verb --> 아/PreEomi \t| target: 윌스미스/Noun\n",
      "윌스미스/Noun 보다/Verb 어린/Verb 윌스미스/Noun --> 의/Josa \t| target: 에/Josa\n",
      "보다/Verb 어린/Verb 윌스미스/Noun 에/Josa --> 대한/Noun \t| target: 발연기/Noun\n",
      "어린/Verb 윌스미스/Noun 에/Josa 발연기/Noun --> 에/Josa \t| target: 는/Josa\n",
      "윌스미스/Noun 에/Josa 발연기/Noun 는/Josa --> 정말/Noun \t| target: 보너스/Noun\n"
     ]
    }
   ],
   "source": [
    "predict_test_sample(test_sent, nnlm, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
