{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Probabilistic Language Model\n",
    "---\n",
    "Paper implementation: \n",
    "\n",
    "* paper: [A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) - Yoshua Bengio, 2003\n",
    "* blog: []()\n",
    "* [slide share](http://bit.ly/2OkYFkY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Preprocessing\n",
    "2. Model\n",
    "3. Result: \n",
    "    * Perplexity\n",
    "    * Similarity versus \"gensim Word2Vec\"\n",
    "    * Training Time\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/'.join(os.getcwd().split('/')[:-1]+['paper_code', 'NNLM']))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from nnlm_data_loader import DataSet\n",
    "from model import NNLM\n",
    "from konlpy.tag import Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = \"cuda\"\n",
    "# USE_CUDA = False\n",
    "# DEVICE = None\n",
    "BATCH = 1024\n",
    "N_GRAM = 5\n",
    "TAGGER = lambda x: ['/'.join(y) for y in Twitter().pos(x, norm=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, valid_data, test_data = DataSet(base_path='../data/nsmc/', train='train.txt', valid='valid.txt',\n",
    "#                                 test='test.txt', n_gram=5, tokenizer=TAGGER, \n",
    "#                                 save_tokens=True, direct_load=False).splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_creator = DataSet(base_path='../data/nsmc/', train='train_tokens', valid='valid_tokens', test='test_tokens',\n",
    "                          n_gram=N_GRAM, tokenizer=str.split, save_tokens=False, \n",
    "                          direct_load=True, remove_short=True, device=DEVICE)\n",
    "train_data, valid_data, test_data = dataset_creator.splits()\n",
    "train_loader, valid_loader, test_loader = dataset_creator.create_loader(train=train_data, valid=valid_data, test=test_data,\n",
    "                                                                        batch_size=BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2167155, 124660)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking removed short sentences which length of tokens is lower then N_GRAM(=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180000, 17727)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual used, number of train sentences: 162273\n",
    "train_data._total, train_data._removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 968)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual used, number of valid sentences: 9032\n",
    "valid_data._total, valid_data._removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is 63202\n"
     ]
    }
   ],
   "source": [
    "# 16383 words : 30 ~~ 63202: 100\n",
    "V = len(train_data.vocab)\n",
    "E = 100\n",
    "H = 500\n",
    "LR = 0.001\n",
    "WD = 0.00001\n",
    "STEP = 5\n",
    "print(\"vocab size is\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnlm = NNLM(embed_size=E, hidden_size=H, vocab_size=V, num_prev_tokens=(N_GRAM-1))\n",
    "if USE_CUDA:\n",
    "    nnlm = nnlm.cuda()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(nnlm.parameters(), lr=LR, weight_decay=WD)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(gamma=0.1, milestones=[2], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(x):\n",
    "    return -torch.log(x).sum() / x.size(0)\n",
    "\n",
    "def validation(model, loader):\n",
    "    model.eval()\n",
    "    pp = 0\n",
    "    acc = 0\n",
    "    for batch in loader:\n",
    "        inputs, targets = batch[0], batch[1]\n",
    "        preds = model.predict(inputs)\n",
    "        probs, idxes = preds.max(1)\n",
    "        acc += torch.eq(idxes, targets).sum().item()\n",
    "        pp += perplexity(probs).item()\n",
    "        \n",
    "    return acc, pp/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5][0/2117] train_loss: 11.0890\n",
      "[1/5][1000/2117] train_loss: 6.5673\n",
      "[1/5][2000/2117] train_loss: 6.2892\n",
      "==============================\n",
      "[1/5]\n",
      " valid_perplextiy: 2.1804 \n",
      " valid_accuracy: 0.1721\n",
      "==============================\n",
      "[2/5][0/2117] train_loss: 5.8295\n",
      "[2/5][1000/2117] train_loss: 5.3505\n",
      "[2/5][2000/2117] train_loss: 5.3846\n",
      "==============================\n",
      "[2/5]\n",
      " valid_perplextiy: 2.0440 \n",
      " valid_accuracy: 0.1905\n",
      "==============================\n",
      "[3/5][0/2117] train_loss: 5.3942\n",
      "[3/5][1000/2117] train_loss: 5.0395\n",
      "[3/5][2000/2117] train_loss: 4.9436\n",
      "==============================\n",
      "[3/5]\n",
      " valid_perplextiy: 2.0824 \n",
      " valid_accuracy: 0.2002\n",
      "==============================\n",
      "[4/5][0/2117] train_loss: 5.1238\n",
      "[4/5][1000/2117] train_loss: 4.8921\n",
      "[4/5][2000/2117] train_loss: 4.8513\n",
      "==============================\n",
      "[4/5]\n",
      " valid_perplextiy: 2.0727 \n",
      " valid_accuracy: 0.2016\n",
      "==============================\n",
      "[5/5][0/2117] train_loss: 5.0296\n",
      "[5/5][1000/2117] train_loss: 4.8180\n",
      "[5/5][2000/2117] train_loss: 4.8003\n",
      "==============================\n",
      "[5/5]\n",
      " valid_perplextiy: 2.0637 \n",
      " valid_accuracy: 0.2020\n",
      "==============================\n",
      "Training Excution time: 17 m 27.0067 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for step in range(STEP):\n",
    "    nnlm.train()\n",
    "    scheduler.step()\n",
    "    losses=[]\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, targets = batch[0], batch[1]\n",
    "\n",
    "        nnlm.zero_grad()\n",
    "\n",
    "        outputs = nnlm(inputs)\n",
    "\n",
    "        loss = loss_function(outputs, targets.view(-1))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(nnlm.parameters(), 50.0)  # gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            msg = '[{}/{}][{}/{}] train_loss: {:.4f}'.format(step+1, STEP, i, len(train_loader), np.mean(losses))\n",
    "            print(msg)\n",
    "            \n",
    "    acc_valid, pp_valid = validation(model=nnlm, loader=valid_loader)\n",
    "    print('='*30)\n",
    "    msg = '[{}/{}]\\n valid_perplextiy: {:.4f} \\n valid_accuracy: {:.4f}'.format(step+1, STEP, pp_valid, acc_valid/len(valid_data))\n",
    "    print(msg)\n",
    "    print('='*30)\n",
    "    \n",
    "end_time = time.time()\n",
    "minute = int((end_time-start_time) // 60)\n",
    "print('Training Excution time with validation: {:d} m {:.4f} s'.format(minute, (end_time-start_time)-minute*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nnlm.state_dict(), '../paper_code/NNLM/model/nnlm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_perplextiy: 2.0619, test_accuracy: 0.2025\n"
     ]
    }
   ],
   "source": [
    "acc, pp = validation(model=nnlm, loader=test_loader)\n",
    "msg = 'test_perplextiy: {:.4f}, test_accuracy: {:.4f}'.format(pp, acc/len(test_data))\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
