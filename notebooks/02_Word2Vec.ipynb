{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec with NEG\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "papers:\n",
    "\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "* [word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method](https://arxiv.org/abs/1402.3722)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/simonjisu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/simonjisu/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "sys.path.append('/'.join(os.getcwd().split('/')[:-1]+['paper_code']))\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torchdata\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from common.vocabulary import Vocab\n",
    "from WORD2VEC.word2vec_dataloader import CustomDataset\n",
    "flatten = lambda d: [tkn for sent in d for tkn in sent ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove \" \\`\\` \" and \" '' \" in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [[tkn.lower() for tkn in sent if tkn not in [\"``\", \"''\"]] for sent in brown.sents()]\n",
    "vocab_counter = Counter(flatten(datas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49813, 57340, 1161192)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_counter), len(datas), len(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = 'cuda' if USE_CUDA else None\n",
    "BATCH = 1024\n",
    "WINDOW_SIZE = 2\n",
    "train_data = CustomDataset(datas, window=WINDOW_SIZE, device=DEVICE)\n",
    "train_loader = torchdata.DataLoader(train_data, batch_size=BATCH, shuffle=True, collate_fn=train_data.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unigram distribution for negative sampling\n",
    "\n",
    "$$(w, c) \\sim p_{words}(w) \\dfrac{p_{contexts} (c)^{3/4} }{Z}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49813 799391\n"
     ]
    }
   ],
   "source": [
    "Z = 10e-5\n",
    "total_words = len(vocab_counter)\n",
    "unigram_distribution = []\n",
    "for w, c in vocab_counter.items():\n",
    "    unigram_distribution.extend([w]*int(((c/total_words)**(3/4))/Z))\n",
    "print(total_words, len(unigram_distribution))\n",
    "unigram_distribution = train_data._numerical(unigram_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(targets, unigram_distribution, k, use_cuda=False):\n",
    "    \"\"\"\n",
    "    unigram_distribution: have to be numericalize words\n",
    "    \"\"\"\n",
    "    neg_samples = []\n",
    "    batch_size = targets.size(0)\n",
    "    for t in targets.view(-1).cpu().tolist():\n",
    "        samples = []\n",
    "        while len(samples) < k:\n",
    "            word = random.choice(unigram_distribution)\n",
    "            assert isinstance(word, int), \"have to be numericalize words\"\n",
    "            if word == t:\n",
    "                continue\n",
    "            samples.append(word)\n",
    "        neg_samples.append(samples)\n",
    "    if use_cuda:\n",
    "        return torch.LongTensor(neg_samples).cuda()\n",
    "    return torch.LongTensor(neg_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/word2vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\begin{aligned}\n",
    "J(\\theta) &= \\dfrac{1}{T}\\sum_{t=1}^{T} J_t(\\theta)\\\\\n",
    "J_t(\\theta) &= \\underbrace{\\log \\sigma(u_o^T v_c)}_{(1)} + \\underbrace{\\sum_{i=1}^{k} \\mathbb{E}_{j \\backsim P(w)} [\\log \\sigma(-u_j^T v_c)]}_{(2)}\n",
    "\\end{aligned}$$\n",
    "\n",
    "* (1) : posivie log score\n",
    "* (2) : negative log score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embedding_w = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        init = (2.0 / (vocab_size + embed_size))**0.5 # Xavier init\n",
    "        nn.init.uniform_(self.embedding_w.weight, -init, init)\n",
    "        nn.init.uniform_(self.embedding_u.weight, -0.0, 0.0)        \n",
    "    \n",
    "    def forward(self, inputs, targets, neg_samples):\n",
    "        embed = self.embedding_w(inputs)  # B, 1, embed_size\n",
    "        context = self.embedding_u(targets)  # B, 1, embed_size\n",
    "        negs = -self.embedding_u(neg_samples)  # B, k, embed_size\n",
    "        \n",
    "        pos = context.bmm(embed.transpose(1, 2)).squeeze(2)  # B, 1\n",
    "        neg = negs.bmm(embed.transpose(1, 2)).sum(1)  # B, k, 1  > B, 1\n",
    "        nll = F.logsigmoid(pos) + F.logsigmoid(neg)\n",
    "        return -torch.mean(nll)\n",
    "    \n",
    "    def cosine_similarity(self, idx1, idx2):\n",
    "        wv1 = self.embedding_w.weight[idx1]\n",
    "        wv2 = self.embedding_w.weight[idx2]\n",
    "        return F.cosine_similarity(wv1, wv2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(train_data.vocab)\n",
    "EMBED = 300\n",
    "K = 10\n",
    "STEP = 60\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(V, EMBED)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(gamma=0.1, milestones=[20, 40], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/60] loss 1.0859\n",
      "[6/60] loss 0.7516\n",
      "[11/60] loss 0.6550\n",
      "[16/60] loss 0.6301\n",
      "[21/60] loss 0.5850\n",
      "[26/60] loss 0.2950\n",
      "[31/60] loss 0.2482\n",
      "[36/60] loss 0.2357\n",
      "[41/60] loss 0.2289\n",
      "[46/60] loss 0.2192\n",
      "[51/60] loss 0.2175\n",
      "[56/60] loss 0.2173\n",
      "Training Excution time with validation: 143 m 22.8557 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "losses = []\n",
    "for step in range(STEP):\n",
    "    scheduler.step()\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        neg_samples = negative_sampling(targets, unigram_distribution, K, use_cuda=USE_CUDA)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss = model(inputs, targets, neg_samples)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if step % 5 == 0:\n",
    "        msg = \"[{}/{}] loss {:.4f}\".format(step+1, STEP, np.mean(losses))\n",
    "        print(msg)\n",
    "        losses = []\n",
    "        \n",
    "end_time = time.time()\n",
    "minute = int((end_time-start_time) // 60)\n",
    "print('Training Excution time with validation: {:d} m {:.4f} s'.format(minute, (end_time-start_time)-minute*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../paper_code/word2vec/model/word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../paper_code/word2vec/model/word2vec.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, model, vocab, top_k=10):\n",
    "    sims = []\n",
    "    for i in range(len(vocab)):\n",
    "        if vocab.itos[i] == word: \n",
    "            continue\n",
    "        sim = model.cosine_similarity(vocab.stoi[word], i)\n",
    "        sims.append((vocab.itos[i], sim.item()))\n",
    "    return sorted(sims, key=lambda x: x[1], reverse=True)[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('he', 0.6608016490936279),\n",
       " ('it', 0.643374502658844),\n",
       " ('you', 0.6358298659324646),\n",
       " ('she', 0.6312659382820129),\n",
       " ('her', 0.6216399073600769),\n",
       " ('they', 0.6160386800765991),\n",
       " ('we', 0.6135802268981934),\n",
       " ('this', 0.6132360696792603),\n",
       " ('that', 0.6096834540367126),\n",
       " ('be', 0.6034596562385559)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(word='i', model=model, vocab=train_data.vocab, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('he', 0.5518719553947449),\n",
       " ('who', 0.5154294967651367),\n",
       " ('him', 0.5083991289138794),\n",
       " ('it', 0.5046495199203491),\n",
       " ('had', 0.5031024813652039),\n",
       " ('one', 0.5026065111160278),\n",
       " ('not', 0.5008120536804199),\n",
       " ('this', 0.49810266494750977),\n",
       " ('you', 0.49751439690589905),\n",
       " ('as', 0.4934142231941223)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(word='man', model=model, vocab=train_data.vocab, top_k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
